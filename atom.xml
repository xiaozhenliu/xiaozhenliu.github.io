<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Live in the Future, then Build What's Missing]]></title>
  <subtitle><![CDATA[刘虓震的技术笔记]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://tech.liuxiaozhen.com/"/>
  <updated>2016-06-12T14:58:41.000Z</updated>
  <id>http://tech.liuxiaozhen.com/</id>
  
  <author>
    <name><![CDATA[Liu Xiaozhen]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Machine Learning By Stanford University Week 5]]></title>
    <link href="http://tech.liuxiaozhen.com/2016/06/12/Machine-Learning-W5/"/>
    <id>http://tech.liuxiaozhen.com/2016/06/12/Machine-Learning-W5/</id>
    <published>2016-06-12T14:49:53.000Z</published>
    <updated>2016-06-12T14:58:41.000Z</updated>
    <content type="html"><![CDATA[<h1 id="5-_Neural_Networks:_Learning">5. Neural Networks: Learning</h1><p>&#x672C;&#x8BFE;&#x56DE;&#x987E;&#x4E86;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x6A21;&#x578B;&#x63CF;&#x8FF0;&#xFF0C;&#x5E76;&#x4ECB;&#x7ECD;&#x4E86;BP&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#x3001;&#x4EE3;&#x4EF7;&#x51FD;&#x6570;&#x8868;&#x8FBE;&#x5F0F;&#x3001;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7B97;&#x6CD5;&#x3001;&#x4EE5;&#x53CA;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x3002;</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course &#x201C;Machine Learning&#x201D; taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="5-1_Neural_Networks_(Classification)">5.1 Neural Networks (Classification)</h2><p>Training set $\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),&#x2026;,(x^{(m)},y^{(m)})\}$</p>
<p><img src="/2016/06/12/Machine-Learning-W5/Coursera-ml-w5-01.png" alt=""></p>
<p>L = total no. of layers in network. (Here L = 4)</p>
<p>$s_l$ = no. of units (not counting bias unit) in layer $l$</p>
<p>(Here $s_1 = 3$, $s_2 = 5$,$s_4 = s_l =4$)</p>
<p>For binary classification, there is only 1 output unit; therefore $s_l = 1$. For multi-class classification (K classes), $s_l = K$. ($K\geq3$) </p>
<h3 id="5-1-1_Cost_functions">5.1.1 Cost functions</h3><h4 id="a)_Logistic_regression">a) Logistic regression</h4><p>$$J(\theta)=-\frac1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log\left(1-h_\theta(x^{(i)})\right)\right] + \frac\lambda{2m}\sum_{j=1}^n\theta_j^2$$</p>
<h4 id="b)_Neural_network">b) Neural network</h4><p>$$h_\Theta(x)\in \mathbb{R}^K\qquad(h_\Theta(x))_i = i^{th}  \space\mathrm{output}$$</p>
<p>$$<br>J(\Theta)=-\frac1m\left[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\right]+\frac\lambda{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2<br>$$</p>
<h3 id="5-1-2_Backpropagation_Algorithm">5.1.2 Backpropagation Algorithm</h3><p>&#x4E3A;&#x4E86;&#x6700;&#x5C0F;&#x5316;&#x8BEF;&#x5DEE;&#xFF0C;&#x9700;&#x8981;&#x627E;&#x5230;&#x4F7F;&#x4EE3;&#x4EF7;&#x51FD;&#x6570; $j(\theta)$ &#x6700;&#x5C0F;&#x7684; $\theta$. &#x56E0;&#x6B64;&#xFF0C;&#x9700;&#x8981;&#x5728;&#x4EE3;&#x7801;&#x4E2D;&#x8BA1;&#x7B97; $j(\theta)$ &#x53CA;&#x5176;&#x5FAE;&#x5206;&#x5F0F;&#x3002;</p>
<h4 id="a)_Gradient_Computation">a) Gradient Computation</h4><p>&#x8003;&#x8651;&#x6700;&#x7B80;&#x5355;&#x60C5;&#x51B5;&#xFF0C;&#x4EC5;&#x6709;&#x4E00;&#x4E2A;&#x8BAD;&#x7EC3;&#x6837;&#x672C;&#xFF1A;<br>Given one training example ($x,y$):</p>
<p><img src="/2016/06/12/Machine-Learning-W5/Coursera-ml-w5-02.png" alt=""></p>
<h4 id="b)_Forward_propagation&#x8BA1;&#x7B97;&#x6B65;&#x9AA4;">b) Forward propagation&#x8BA1;&#x7B97;&#x6B65;&#x9AA4;</h4><ul>
<li>$a^{(1)} = x$</li>
<li>$z^{(2)} = \Theta^{(1)}a^{(1)}$</li>
<li>$a^{(2)}=g(z^{(2)})\quad(\mathrm{add}\space a_0^{(2)})$</li>
<li>$z^{(3)} = \Theta^{(2)}a^{(2)}$</li>
<li>$a^{(3)}=g(z^{(3)})\quad(\mathrm{add}\space a_0^{(3)})$</li>
<li>$z^{(4)} = \Theta^{(3)}a^{(3)}$</li>
<li>$a^{(4)}=h_\Theta(x)=g(z^{(4)})$</li>
</ul>
<h4 id="c)_Backpropagation_algorithm">c) Backpropagation algorithm</h4><p>Intuition: $\delta_j^{(l)} = $ &#x201C;error&#x201D; of node $j$ in layer $l$.</p>
<p>&#x8BBE; $\delta_j^{(l)}$ &#x4E3A;&#x7B2C; $l$ &#x5C42;&#x7B2C; $j$ &#x4E2A;&#x795E;&#x7ECF;&#x5143;&#x7684;&#x8BEF;&#x5DEE;&#x503C;&#x3002;</p>
<p>For each output unit (layer L = 4)</p>
<ul>
<li>$\delta_j^{(4)}=a_j^{(4)}-y_j=(h_\Theta(x))_j-y_j$</li>
</ul>
<p>Vector form:</p>
<ul>
<li><p>$\delta^{(4)}=a^{(4)}-y$</p>
</li>
<li><p>$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}._g&#x2019;(z^{(3)}),\quad g&#x2019;(z^{(3)})=a^{(3)}._(1-a^{(3)})$</p>
</li>
<li><p>$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}._g&#x2019;(z^{(2)}),\quad g&#x2019;(z^{(2)})=a^{(2)}._(1-a^{(2)})$</p>
</li>
<li><p>(There is No $\delta^{(1)}$!)</p>
</li>
</ul>
<h4 id="d)_Backpropagation_implementation">d) Backpropagation implementation</h4><ul>
<li><p>Set $\Delta_{ij}^{(l)}=0$ (for all $l,i,j$).</p>
</li>
<li><p>For $i = 1$ to $m$</p>
<ul>
<li>&#x8BBE;&#x7F6E;&#x521D;&#x59CB;&#x6FC0;&#x6D3B;&#x503C; Set $a^{(1)}=x^{(i)}$</li>
<li>&#x6B63;&#x5411;&#x8BA1;&#x7B97;&#x5404;&#x5C42;&#x6FC0;&#x6D3B;&#x503C; Perform forward propagation to compute $a^{(l)}$ for $l=2,3,&#x2026;,L$</li>
<li>&#x7528;&#x8F93;&#x51FA;&#x7ED3;&#x679C;&#xFF0C;&#x8BA1;&#x7B97;&#x6700;&#x7EC8;&#x5C42;&#x8BEF;&#x5DEE; Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$</li>
<li>&#x53CD;&#x5411;&#x8BA1;&#x7B97;&#x5176;&#x4ED6;&#x5C42;&#x7684;&#x8BEF;&#x5DEE; Compute $\delta^{(L-1)}, \delta^{(L-2)},\delta^{(2)}$</li>
<li><p>$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$   </p>
<p> Here, vector form: </p>
<p> $\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$  </p>
</li>
</ul>
</li>
</ul>
<p>  $$\frac\partial{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$$</p>
<ul>
<li>&#x4EE3;&#x4EF7;&#x51FD;&#x6570;&#x7684;&#x5FAE;&#x5206;&#x5F0F;<br>$D_{ij}^{(l)}:=\frac1m\Delta_{ij}^{(l)}+\lambda\Delta_{ij}^{(l)}$ if $j\neq0$<br>$D_{ij}^{(l)}:= \frac1m\Delta_{ij}^{(l)}\qquad$ if $j=0$</li>
</ul>
<h3 id="5-1-3_Backpropagation_Intuition">5.1.3 Backpropagation Intuition</h3><h4 id="a)_Forward_Propagation">a) Forward Propagation</h4><p><img src="/2016/06/12/Machine-Learning-W5/Coursera-ml-w5-03.png" alt=""></p>
<h4 id="b)_What_is_backpropagation_doing">b) What is backpropagation doing</h4><p>Focosing on a single example $x^{(i)}, y^{(i)}$, the case of 1 output unit, and ignoring regularization ($\lambda=0$),<br>$$<br>cost(i)=y^{(i)}\log h_\Theta(x^{(i)})+(1-y^{(i)})\log h_\Theta(x^{(i)})<br>$$</p>
<p>(Think of $cost(i)\approx(h_\Theta(x^{(i)})-y^{(i)})^2$)</p>
<p>i.e. how well is the network doing on example i?</p>
<p><img src="/2016/06/12/Machine-Learning-W5/Coursera-ml-w5-04.png" alt=""></p>
<h2 id="5-2_Backpropagation_Implementation_Notes">5.2 Backpropagation Implementation Notes</h2><h3 id="5-2-1_Unrolling_Parameters">5.2.1 Unrolling Parameters</h3><h4 id="a)_Advanced_Optimization">a) Advanced Optimization</h4><p><code>function [jVal, gradient] = costFunction(theta)</code><br>&#x2026;<br><code>optTheta = fminunc(@costFunction, initialTheta, options)</code></p>
<p>Neutal Network(L=4):<br>$\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$ - matrices(<code>Theta1, Theta2, Theta3</code>)<br>$D^{(1)},D^{(2)},D^{(3)}$ - matrices (<code>D1, D2, D3</code>)</p>
<p>&#x201C;Unroll&#x201D; into vectors</p>
<h4 id="b)_Example">b) Example</h4><p>$s_1 = 10,s_2 = 10, s_3=1$</p>
<p>$\Theta^{(1)}\in \mathbb{R}^{10\times11}, \Theta^{(2)}\in \mathbb{R}^{10\times11},\Theta^{(3)}\in \mathbb{R}^{1\times11}$</p>
<p>$D^{(1)}\in \mathbb{R}^{10\times11}, D^{(2)}\in \mathbb{R}^{10\times11},D^{(3)}\in \mathbb{R}^{1\times11}$</p>
<p><code>thetaVec = [ Theta1(:); THeta2(:); Theta3(:)];</code><br><code>DVec = [D1(:); D2(:); D3(:)];</code></p>
<p><code>Theta1 = reshape(thetaVec(1:110),10,11);</code><br><code>Theta2 = reshape(thetaVec(111:220),10,11);</code><br><code>Theta3 = reshape(thetaVec(221:231),1,11);</code></p>
<h4 id="c)_Learning_Algorithm">c) Learning Algorithm</h4><ul>
<li><p>Have initial parameters $\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$.</p>
</li>
<li><p>Unroll to get <code>initialTheta</code> to pass to <code>fminunc(@costFunction, initialTheta, options)</code></p>
</li>
<li><p><code>function [jval, gradientVec] = costFunction(thetaVec)</code></p>
</li>
<li><p>From <code>thetaVec</code>, get $\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$.</p>
</li>
<li><p>Use forward prop/back prop to compute $D^{(1)},D^{(2)},D^{(3)}$ and $J(\Theta)$.</p>
</li>
<li>Unroll $D^{(1)},D^{(2)},D^{(3)}$ to get <code>gradientVec</code>.</li>
</ul>
<h3 id="5-2-2_Gradient_Checking">5.2.2 Gradient Checking</h3><p>In order to verify that the neural network algorithm is working properly, one needs to do gradient checking. </p>
<h4 id="a)_Numerial_estimation_of_gradients">a) Numerial estimation of gradients</h4><p><img src="/2016/06/12/Machine-Learning-W5/Coursera-ml-w5-05.png" alt=""></p>
<p>Implement:<br><code>gradApprox = (J(theta + EPASILON) - J(theta - EPSILON))/(2*EPSILON)</code></p>
<h4 id="b)_Parameter_vector_$\theta$">b) Parameter vector $\theta$</h4><p><img src="/2016/06/12/Machine-Learning-W5/Coursera-ml-w5-06.png" alt=""></p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">	thetaPlus = theta;</span><br><span class="line">	thetaPlus(<span class="built_in">i</span>) = thetaPlus(<span class="built_in">i</span>) + EPSILON;</span><br><span class="line">	thetaMinus = theta;</span><br><span class="line">	thetaMinus(<span class="built_in">i</span>) = thetaMinus(<span class="built_in">i</span>) - EPSILON;</span><br><span class="line">	gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*EPSILON);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p>Check that <code>gradApprox</code> $\approx$ <code>DVec</code> </p>
<h4 id="c)_Implementation_Note:">c) Implementation Note:</h4><ul>
<li>Implement backprop to compute <code>DVec</code> (unrolled $D^{(1)},D^{(2)},D^{(3)}$).</li>
<li>Implement numerial gradient check to compute <code>gradApprox</code>.</li>
<li>Make sure they give similar values.</li>
<li>Turn off gradient checking. Using backprop code for learning.</li>
</ul>
<p><strong>Important:</strong></p>
<ul>
<li>Be sure to disable your gradient cheking code before training your classifier. If you run numeiral gradient computation on every iteration of gradient descent (or in the inner loop of <code>costFunction(...)</code>) your code will be very slow.</li>
</ul>
<h3 id="5-2-3_Random_Initialization">5.2.3 Random Initialization</h3><p>Need initial value for $\Theta\$</p>
<ul>
<li>Zero initialization. <ul>
<li>$\Theta_{ij}^{(l)} = 0$ for all $i,j,l$</li>
<li>will cause the problem of symmetric ways: </li>
</ul>
</li>
<li>Random initialization: Symmetry breaking. <ul>
<li>initialize each $\Theta_{ij}^{(l)}$ to a random value in $[-\epsilon,\epsilon]$</li>
<li>E.g.<br><code>Theta1 = rand(10,11) * (2*INIT_EPSILON) - INIT_EPSILON;</code><br><code>Theta2 = rand(1,11) * (2*INIT_EPSILON) - INIT_EPSILON;</code></li>
</ul>
</li>
</ul>
<h2 id="5-3_Putting_It_Together">5.3 Putting It Together</h2><h3 id="5-3-1_Training_a_neural_network">5.3.1 Training a neural network</h3><ul>
<li>pick a network architecture (connectivity pattern between neurons)</li>
<li>No. of input units: Dimension of features </li>
<li>No. output units: Number of classes</li>
<li>Reasonable default: 1 hidden layer, or if &gt;1 hidden layer, have same no. of hidden uints in every layer (usually the more the better, but will be more computational expensive). </li>
</ul>
<h3 id="5-3-2_Steps_of_training_a_neural_network">5.3.2 Steps of training a neural network</h3><ol>
<li>Randomly initialize weights</li>
<li>Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$</li>
<li>Implement code to compute cost function $J(\Theta)$</li>
<li>Implement backprop to compute partial derivatives $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$</li>
<li>Use gradient checking to compare $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$ computed using backpropagation v.s. using numerical estimate of gradient of $J(\Theta)$. Then disable gradient checking code.</li>
<li>Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\theta)$ as a function of parameters $\Theta$</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<h1 id="5-_Neural_Networks:_Learning">5. Neural Networks: Learning</h1><p>&#x672C;&#x8BFE;&#x56DE;&#x987E;&#x4E86;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x6A21;&#x578B;&#x63CF;&#x8FF0;&#xFF0C;&#x5E76;&#x4ECB;&#x7ECD;&#x4E86;BP&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#x3001;&#x4EE3;&#x4EF7;&#x51FD;&#x6570;&#x8868;&#x8FBE;&#x5F0F;&#x3001;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x7B97;&#x6CD5;&#x3001;&#x4EE5;&#x53CA;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x3002;</p>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据分析师养成计划（2）]]></title>
    <link href="http://tech.liuxiaozhen.com/2016/04/28/udacity-data-analyst-nanodegree-1/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/28/udacity-data-analyst-nanodegree-1/</id>
    <published>2016-04-28T08:59:53.000Z</published>
    <updated>2016-05-06T08:05:34.000Z</updated>
    <content type="html"><![CDATA[<p>&#x8BB0;&#x5F55;Udacity&#x4E0A;&#x7684;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;Nanodegree&#x7684;&#x7B2C;&#x4E00;&#x90E8;&#x5206;&#x8BFE;&#x7A0B;&#x5185;&#x5BB9;&#xFF0C;&#x5305;&#x62EC;&#x6700;&#x57FA;&#x672C;&#x7684;&#x7EDF;&#x8BA1;&#x5B66;&#x77E5;&#x8BC6;&#x3002;</p>
<a id="more"></a>
<p><strong>&#x672C;&#x6587;&#x662F;&#x5B66;&#x4E60;Udacity&#x4E0A;&#x7684;Nanodegree: Data Analyst&#x7684;&#x8BB0;&#x5F55;&#xFF0C;&#x81EA;2016&#x5E74;4&#x6708;24&#x65E5;&#x59CB;&#x3002;</strong></p>
<p><strong>&#x672C;&#x7CFB;&#x5217;&#x5168;&#x90E8;&#x6587;&#x7AE0;&#x89C1;<a href="http://tech.liuxiaozhen.com/tags/Data-Analyst/" target="_blank" rel="external">Tag: Data Analyst</a> &#x3002;</strong></p>
<h1 id="Project_1">Project 1</h1><h2 id="Lesson_1_Intro_to_Research_Methods">Lesson 1 Intro to Research Methods</h2><p>&#x8FD9;&#x8282;&#x8BFE;&#x8BB2;&#x600E;&#x4E48;&#x505A;&#x8C03;&#x67E5;&#x7814;&#x7A76;&#x3002;&#x5B9A;&#x6027;&#x5730;&#x8BB2;&#x4E86;&#x91C7;&#x6837;&#xFF0C;&#x968F;&#x673A;&#x6027;&#x3001;&#x53CC;&#x76F2;&#x5B9E;&#x9A8C;&#x3001;&#x76F8;&#x5173;&#x6027;&#x4E0E;&#x56E0;&#x679C;&#x6027;&#xFF0C;&#x6570;&#x636E;&#x9610;&#x91CA;&#x7B49;&#x3002;</p>
<p>&#x5173;&#x952E;&#x6982;&#x5FF5;&#xFF1A;</p>
<ul>
<li>Population &amp; Sample</li>
<li>Parameters &amp; Statistics</li>
<li>Sampling Error</li>
<li>Independent Variables, Dependent Variables, and Lurking Variables</li>
<li>Observations v.s. Experiments</li>
<li>&#x76F8;&#x5173;&#x6027;&#x4E0E;&#x56E0;&#x679C;&#x6027;</li>
<li>&#x53CC;&#x76F2;&#x5B9E;&#x9A8C;</li>
</ul>
<h2 id="Lesson_2:_Visualizing_Data">Lesson 2: Visualizing Data</h2><p>&#x8FD9;&#x8282;&#x8BFE;&#x8BB2;&#x5982;&#x4F55;&#x7528;&#x76F4;&#x65B9;&#x56FE;&#x6765;&#x5448;&#x73B0;&#x6570;&#x636E;&#x3002;</p>
<p>&#x5173;&#x952E;&#x6982;&#x5FF5;&#xFF1A;</p>
<ul>
<li>Frequency, Relative Frequency, Proportions and Percentages</li>
<li>Histogram: numerical values</li>
<li>Bar Graph: categories</li>
<li>Bin size</li>
<li>Skewed Distribution<ul>
<li>Positively skewed distribution: scores with the lowest frequencies are on the right side of the distribution</li>
<li>Negatively skewed distribution: scores with the highest frequencies are on the left side of the distribution</li>
</ul>
</li>
<li>Uniform, bimode and normal distrubution</li>
</ul>
<h2 id="Lesson_3:_Central_Tendancy">Lesson 3: Central Tendancy</h2><p>&#x8FD9;&#x8282;&#x8BFE;&#x4E3B;&#x8981;&#x8BB2;mode, mean, median&#x7684;&#x6982;&#x5FF5;&#xFF0C;&#x4EE5;&#x53CA;&#x5BF9;&#x4E8E;&#x4E0D;&#x540C;&#x7684;&#x5206;&#x5E03;&#xFF0C;&#x8FD9;&#x4E9B;&#x7279;&#x5F81;&#x503C;&#x6709;&#x4F55;&#x7279;&#x70B9;&#xFF0C;&#x53CD;&#x6620;&#x4E86;&#x4EC0;&#x4E48;&#x6837;&#x7684;&#x6027;&#x8D28;&#x3002;</p>
<h2 id="Lesson_4:_Variability">Lesson 4: Variability</h2><p>&#x8FD9;&#x8282;&#x8BFE;&#x8BB2;&#x4E86;variance&#x548C;standard deviation&#x5982;&#x4F55;&#x8BA1;&#x7B97;&#xFF0C;&#x4EE5;&#x53CA;normal distribution&#x7684;&#x4E00;&#x4E9B;&#x5C5E;&#x6027;&#x3002;</p>
<p>&#x76EE;&#x524D;&#x4E3A;&#x6B62;&#x8BFE;&#x7A0B;&#x90FD;&#x8FD8;&#x5F88;&#x57FA;&#x7840;&#xFF0C;&#x6682;&#x65F6;&#x4E0D;&#x4F1A;&#x6709;&#x4EC0;&#x4E48;&#x5927;&#x620F;&#x3002;&#x6211;&#x4EEC;&#x8FD8;&#x662F;&#x6765;&#x770B;&#x770B;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5427;~<br>&#xFF08;&#x5F85;&#x7EED;&#xFF09;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>&#x8BB0;&#x5F55;Udacity&#x4E0A;&#x7684;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;Nanodegree&#x7684;&#x7B2C;&#x4E00;&#x90E8;&#x5206;&#x8BFE;&#x7A0B;&#x5185;&#x5BB9;&#xFF0C;&#x5305;&#x62EC;&#x6700;&#x57FA;&#x672C;&#x7684;&#x7EDF;&#x8BA1;&#x5B66;&#x77E5;&#x8BC6;&#x3002;</p>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据分析师养成计划（1）]]></title>
    <link href="http://tech.liuxiaozhen.com/2016/04/27/udacity-data-analyst-nanodegree-0/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/27/udacity-data-analyst-nanodegree-0/</id>
    <published>2016-04-27T09:17:30.000Z</published>
    <updated>2016-05-06T08:03:33.000Z</updated>
    <content type="html"><![CDATA[<p>&#x4ECB;&#x7ECD;Udacity&#xFF08;&#x5728;&#x534E;&#x540D;&#x79F0;&#x201C;&#x4F18;&#x8FBE;&#x5B66;&#x57CE;&#x201D;&#xFF09;&#x7684;&#x201C;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x201D;&#x9879;&#x76EE;&#xFF0C;&#x53CA;&#x5176;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;&#x9879;&#x76EE;&#x7684;&#x57FA;&#x672C;&#x60C5;&#x51B5;&#x3002;</p>
<a id="more"></a>
<p><strong>&#x672C;&#x6587;&#x662F;&#x5B66;&#x4E60;Udacity&#x4E0A;&#x7684;Nanodegree: Data Analyst&#x7684;&#x8BB0;&#x5F55;&#xFF0C;&#x81EA;2016&#x5E74;4&#x6708;24&#x65E5;&#x59CB;&#x3002;</strong></p>
<p><strong>&#x672C;&#x7CFB;&#x5217;&#x5168;&#x90E8;&#x6587;&#x7AE0;&#x89C1;<a href="http://tech.liuxiaozhen.com/tags/Data-Analyst/" target="_blank" rel="external">Tag: Data Analyst</a> &#x3002;</strong></p>
<h1 id="&#x7B80;&#x4ECB;">&#x7B80;&#x4ECB;</h1><p>&#x5728;&#x7EBF;&#x5B66;&#x4E60;&#x5E73;&#x53F0;Udacity&#x53BB;&#x5E74;&#x521A;&#x521A;&#x5B8C;&#x6210;D&#x8F6E;&#x878D;&#x8D44;&#xFF0C;&#x4F30;&#x503C;&#x8D85;&#x8FC7;10&#x4EBF;&#x7F8E;&#x5143;&#xFF0C;&#x6210;&#x4E3A;&#x5728;&#x7EBF;&#x6559;&#x80B2;&#x7B2C;&#x4E00;&#x5BB6;&#x72EC;&#x89D2;&#x517D;&#x3002;&#x6211;&#x4ECE;2012&#x5E74;&#x5C31;&#x5F00;&#x59CB;&#x5173;&#x6CE8;&#x548C;&#x5B66;&#x4E60;Udacity&#x7684;&#x8BFE;&#x7A0B;&#x3002;&#x4ECA;&#x5E74;4&#x6708;&#xFF0C;Udacity&#x4EE5;&#x201C;&#x4F18;&#x8FBE;&#x5B66;&#x57CE;&#x201D;&#x4E3A;&#x540D;&#x5165;&#x534E;&#xFF0C;&#x4E00;&#x8FDB;&#x5165;&#x5C31;&#x8DDF;&#x6EF4;&#x6EF4;&#x3001;&#x4EAC;&#x4E1C;&#x3001;&#x65B0;&#x6D6A;&#x8FBE;&#x6210;&#x5408;&#x4F5C;&#x610F;&#x5411;&#xFF0C;&#x4E3A;&#x8FD9;&#x4E9B;&#x4F01;&#x4E1A;&#x5BFB;&#x627E;&#x548C;&#x57F9;&#x517B;&#x4EBA;&#x624D;&#x3002;</p>
<p>&#x201C;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x201D;&#xFF08;Nanodegreee, ND&#xFF09;&#x662F;Udacity&#x76EE;&#x524D;&#x7684;&#x4E3B;&#x6253;&#x4EA7;&#x54C1;&#xFF0C;&#x610F;&#x56FE;&#x8BA9;&#x5B66;&#x5458;&#x901A;&#x8FC7;9-12&#x4E2A;&#x6708;&#x7684;&#x4E1A;&#x4F59;&#x5B66;&#x4E60;&#xFF08;&#x6BCF;&#x6708;&#x81F3;&#x5C11;10&#x5C0F;&#x65F6;&#x6295;&#x5165;&#xFF09;&#x6210;&#x4E3A;&#x80FD;&#x591F;&#x80DC;&#x4EFB;&#x67D0;&#x4E2A;IT&#x6280;&#x672F;&#x5C97;&#x4F4D;&#x7684;&#x4EBA;&#x624D;&#xFF08;&#x521D;&#x7EA7;&#x4E3A;&#x4E3B;&#xFF09;&#x3002;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x7684;&#x6536;&#x8D39;&#x4EE5;&#x6708;&#x4E3A;&#x5355;&#x4F4D;&#xFF0C;&#x82F1;&#x6587;&#x7F51;&#x7AD9;&#x4E00;&#x822C;&#x662F;$199/&#x6708;&#xFF0C;&#x5728;&#x534E;&#x4EF7;&#x683C;&#x662F;&#xFFE5;980/&#x6708;&#xFF0C;&#x8FD9;&#x4E2A;&#x4EF7;&#x683C;&#x4E3B;&#x8981;&#x7528;&#x4E8E;&#x652F;&#x4ED8;&#x4E00;&#x5BF9;&#x4E00;&#x8F85;&#x5BFC;&#xFF0C;&#x56E0;&#x4E3A;&#x8BFE;&#x7A0B;&#x5185;&#x5BB9;&#x662F;&#x514D;&#x8D39;&#x7684;&#x3002;</p>
<p>&#x4E0D;&#x8FC7;&#xFF0C;&#x4F60;&#x5E76;&#x4E0D;&#x9700;&#x8981;&#x4E00;&#x4E07;&#x591A;&#x5143;&#x624D;&#x80FD;&#x83B7;&#x5F97;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#xFF0C;&#x56E0;&#x4E3A;Udacity&#x7684;&#x8003;&#x6838;&#x662F;&#x901A;&#x8FC7;&#x8BC4;&#x4F30;&#x4F60;&#x63D0;&#x4EA4;&#x7684;&#x6307;&#x5B9A;&#x9879;&#x76EE;&#x6765;&#x5B8C;&#x6210;&#x3002;&#x4F60;&#x5B8C;&#x5168;&#x53EF;&#x4EE5;&#x5148;&#x81EA;&#x5B66;&#x6240;&#x6709;&#x5185;&#x5BB9;&#xFF0C;&#x505A;&#x597D;&#x9879;&#x76EE;&#xFF0C;&#x518D;&#x4ED8;&#x8D39;&#x5B66;&#x4E60;&#x3002;&#x5229;&#x7528;&#x201C;&#x4E00;&#x5E74;&#x5185;&#x5B8C;&#x6210;&#x8FD4;&#x8FD8;&#x4E00;&#x534A;&#x5B66;&#x8D39;&#x201D;&#x7684;&#x653F;&#x7B56;&#xFF0C;&#x6700;&#x4F4E;&#x53EA;&#x9700;&#x534A;&#x4E2A;&#x6708;&#x7684;&#x4EF7;&#x683C;&#x5C31;&#x53EF;&#x4EE5;&#x83B7;&#x5F97;&#x201C;&#x5B66;&#x4F4D;&#x8BC1;&#x201D;&#x3002;</p>
<p><strong>&#x552F;&#x4E00;&#x7684;&#x95EE;&#x9898;&#x662F;&#xFF1A;&#x4F60;&#x80FD;&#x575A;&#x6301;&#x5B66;&#x4E60;&#x5417;&#xFF1F;</strong></p>
<p>Udacity&#x73B0;&#x6709;&#x5341;&#x4F59;&#x4E2A;&#x201C;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x201D;&#xFF0C;&#x5305;&#x62EC;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;&#x3001;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5DE5;&#x7A0B;&#x5E08;&#x3001;&#x524D;&#x7AEF;&#x5DE5;&#x7A0B;&#x5E08;&#x3001;&#x540E;&#x7AEF;&#x5DE5;&#x7A0B;&#x5E08;&#x3001;iOS&#x5E94;&#x7528;&#x5F00;&#x53D1;&#x3001;&#x5B89;&#x5353;&#x5E94;&#x7528;&#x5F00;&#x53D1;&#x5DE5;&#x7A0B;&#x5E08;&#x7B49;&#x3002;&#x90E8;&#x5206;&#x8BFE;&#x7A0B;&#x5DF2;&#x7ECF;&#x88AB;&#x8BD1;&#x6210;&#x4E2D;&#x6587;&#x3002;</p>
<h1 id="&#x8BFE;&#x524D;&#x51C6;&#x5907;">&#x8BFE;&#x524D;&#x51C6;&#x5907;</h1><p>&#x6211;&#x7684;&#x6570;&#x636E;&#x5206;&#x6790;&#x57FA;&#x7840;&#x8FD8;&#x4E0D;&#x9519;&#xFF0C;&#x6240;&#x4EE5;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;&#x8FD9;&#x4E2A;&#x4E00;&#x534A;&#x8BFE;&#x7A0B;&#x770B;&#x8D77;&#x6765;&#x90FD;&#x773C;&#x719F;&#x7684;ND&#x5C31;&#x6210;&#x4E86;&#x6211;&#x7684;&#x4E0A;&#x624B;&#x730E;&#x7269;&#x2026;&#x2026;</p>
<p>Udacity&#x6539;&#x7248;&#x4E4B;&#x540E;&#xFF0C;&#x589E;&#x52A0;&#x4E86;&#x4E0D;&#x5C11;&#x8D34;&#x5FC3;&#x7684;&#x529F;&#x80FD;&#xFF0C;&#x6BD4;&#x5982;&#x628A;&#x67D0;&#x4E2A;Nanodegree&#x9700;&#x8981;&#x591A;&#x5C11;&#x65F6;&#x95F4;&#x5B8C;&#x6210;&#x90FD;&#x8BA1;&#x7B97;&#x597D;&#xFF0C;&#x751A;&#x81F3;&#x8FDE;&#x6BCF;&#x4E00;&#x8282;&#x8BFE;&#x7684;&#x65F6;&#x95F4;&#x90FD;&#x663E;&#x793A;&#x5728;&#x5B66;&#x4E60;&#x8BA1;&#x5212;&#x4E2D;&#xFF1B;&#x786E;&#x5B9E;&#x4E3A;&#x4E1A;&#x4F59;&#x5B66;&#x4E60;&#x63D0;&#x4F9B;&#x4E86;&#x4E0D;&#x5C11;&#x4FBF;&#x5229;&#x3002;</p>
<p>&#x7136;&#x540E;&#x6211;&#x5C31;&#x5728;&#x9996;&#x6708;&#x514D;&#x8D39;&#x7684;&#x86CA;&#x60D1;&#x4E0B;&#x8BA2;&#x9605;&#x4E86;&#x8FD9;&#x4E2A;&#x5B66;&#x4F4D;&#x5566;&#x3002;&#x63A5;&#x7740;&#x5C31;&#x4E0D;&#x77E5;&#x4E0D;&#x89C9;&#x5730;&#x770B;&#x5B8C;&#x4E86;&#x7B2C;&#x4E00;&#x8BFE;&#x3002;&#x55EF;&#xFF0C;&#x8FD9;&#x4E0D;&#x662F;&#x7EDF;&#x8BA1;&#x5B66;101&#x7684;&#x5185;&#x5BB9;&#x561B;&#xFF0C;&#x771F;&#x7684;&#x5B8C;&#x5168;&#x65E0;&#x538B;&#x529B;&#x597D;&#x5417;&#xFF1F;</p>
<p>&#x5728;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x7684;&#x6B22;&#x8FCE;&#x89C6;&#x9891;&#x4E2D;&#xFF0C;&#x4E00;&#x4F4D;&#x77E5;&#x5FC3;&#x5927;&#x59D0;&#x544A;&#x8BC9;&#x6211;&#x4EEC;&#xFF1A;&#x65F6;&#x95F4;&#x6295;&#x5165;&#x662F;&#x6210;&#x529F;&#x5B8C;&#x6210;&#x5B66;&#x4F4D;&#x7684;&#x6700;&#x91CD;&#x8981;&#x56E0;&#x7D20;&#xFF0C;&#x4E00;&#x5B9A;&#x8981;&#x5728;&#x81EA;&#x5DF1;&#x7684;&#x65E5;&#x7A0B;&#x4E2D;&#x9884;&#x7559;&#x51FA;&#x5B66;&#x4E60;&#x65F6;&#x95F4;&#x3002;Udacity&#x63A8;&#x8350;&#x6BCF;&#x5468;&#x81F3;&#x5C11;&#x82B1;10&#x5C0F;&#x65F6;&#x5B66;&#x4E60;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#xFF0C;&#x4E14;&#x6700;&#x597D;&#x4E0D;&#x8981;&#x4E00;&#x6B21;&#x6027;&#x5B66;&#x4E60;10&#x5C0F;&#x65F6;&#xFF0C;&#x5206;&#x6563;&#x5230;&#x6BCF;&#x5929;&#x8F83;&#x597D;&#x3002;</p>
<p>&#x6211;&#x7B97;&#x4E86;&#x7B97;&#xFF0C;&#x6BCF;&#x5929;&#x5B66;&#x4E60;3-4&#x5C0F;&#x65F6;&#x662F;&#x80FD;&#x4FDD;&#x8BC1;&#x7684;&#x3002;&#x8FD9;&#x6837;&#x4E00;&#x5468;20&#x5C0F;&#x65F6;&#x90FD;&#x6709;&#x4F59;&#x4E86;&#x3002;&#x4E8E;&#x662F;&#x624B;&#x4E00;&#x6296;&#xFF0C;&#x53C8;&#x8BA2;&#x9605;&#x4E86;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7684;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x3002;&#x82B1;&#x5F00;&#x4E24;&#x6735;&#xFF0C;&#x5404;&#x8868;&#x4E00;&#x679D;&#xFF0C;&#x5173;&#x4E8E;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x7684;ND&#x53E6;&#x6587;&#x9610;&#x8FF0;&#x3002;</p>
<p>&#x4E0B;&#x9762;&#x5C31;&#x662F;&#x5B66;&#x4E60;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x7684;&#x8BB0;&#x5F55;&#x3002;</p>
<h1 id="Project_0">Project 0</h1><p>&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x9009;&#x505A;&#x9879;&#x76EE;&#xFF0C;&#x901A;&#x8FC7;&#x505A;&#x8FD9;&#x4E2A;&#x9879;&#x76EE;&#x53EF;&#x4EE5;&#x4E0A;&#x624B;&#x672A;&#x6765;&#x672C;&#x8BFE;&#x7A0B;&#x6240;&#x9700;&#x7684;&#x5DE5;&#x5177;&#x3002;</p>
<h2 id="Python&#x6570;&#x636E;&#x5206;&#x6790;&#x5F00;&#x53D1;&#x73AF;&#x5883;&#xFF1A;Anaconda">Python&#x6570;&#x636E;&#x5206;&#x6790;&#x5F00;&#x53D1;&#x73AF;&#x5883;&#xFF1A;Anaconda</h2><p>&#x505A;&#x8FD9;&#x4E2A;&#x9879;&#x76EE;&#x9996;&#x5148;&#x8981;&#x5B89;&#x88C5;<a href="https://www.continuum.io/downloads" target="_blank" rel="external"> Anaconda </a>&#xFF0C;&#x4EE5;&#x4FBF;&#x6109;&#x5FEB;&#x5730;&#x4F7F;&#x7528;Python&#x3002;&#x4E0B;&#x8F7D;&#x5B89;&#x88C5;&#x81EA;&#x4E0D;&#x7528;&#x63D0;&#x3002;&#x5B98;&#x65B9;&#x63D0;&#x4F9B;&#x7684;<a href="http://conda.pydata.org/docs/test-drive.html#managing-conda" target="_blank" rel="external"> Test Drive </a>&#x4F1A;&#x6559;&#x4E00;&#x4E9B;&#x57FA;&#x672C;&#x914D;&#x7F6E;&#x65B9;&#x6CD5;&#x3002;</p>
<h2 id="Python&#x4EA4;&#x4E92;&#x8BA1;&#x7B97;&#x5E73;&#x53F0;&#xFF1A;IPython_Notebook">Python&#x4EA4;&#x4E92;&#x8BA1;&#x7B97;&#x5E73;&#x53F0;&#xFF1A;IPython Notebook</h2><p>&#x672C;&#x9879;&#x76EE;&#x7528;&#x5230;&#x4E86;IPython Notebook&#x3002;</p>
<p>&#x8FD9;&#x91CC;<a href="http://mindonmind.github.io/2013/02/08/ipython-notebook-interactive-computing-new-era/" target="_blank" rel="external">&#x5F15;&#x7528;&#x4E00;&#x6BB5;</a>&#x5BF9;IPython Notebook&#x7684;&#x4ECB;&#x7ECD;&#xFF1A;</p>
<blockquote>
<p>IPython Notebook &#x65E2;&#x662F;&#x4E00;&#x4E2A;&#x4EA4;&#x4E92;&#x8BA1;&#x7B97;&#x5E73;&#x53F0;&#xFF0C;&#x53C8;&#x662F;&#x4E00;&#x4E2A;&#x8BB0;&#x5F55;&#x8BA1;&#x7B97;&#x8FC7;&#x7A0B;&#x7684;&#x300C;&#x7B14;&#x8BB0;&#x672C;&#x300D;&#x3002;&#x5B83;&#x7531;&#x670D;&#x52A1;&#x7AEF;&#x548C;&#x5BA2;&#x6237;&#x7AEF;&#x4E24;&#x90E8;&#x5206;&#x7EC4;&#x6210;&#xFF0C;&#x5176;&#x4E2D;&#x670D;&#x52A1;&#x7AEF;&#x8D1F;&#x8D23;&#x4EE3;&#x7801;&#x7684;&#x89E3;&#x91CA;&#x4E0E;&#x8BA1;&#x7B97;&#xFF0C;&#x800C;&#x5BA2;&#x6237;&#x7AEF;&#x8D1F;&#x8D23;&#x4E0E;&#x7528;&#x6237;&#x8FDB;&#x884C;&#x4EA4;&#x4E92;&#x3002;&#x670D;&#x52A1;&#x7AEF;&#x53EF;&#x4EE5;&#x8FD0;&#x884C;&#x5728;&#x672C;&#x673A;&#x4E5F;&#x53EF;&#x4EE5;&#x8FD0;&#x884C;&#x5728;&#x8FDC;&#x7A0B;&#x670D;&#x52A1;&#x5668;&#xFF0C;&#x5305;&#x542B;&#x8D1F;&#x8D23;&#x8FD0;&#x7B97;&#x7684; IPython kernel (&#x4E0E; QT Console &#x7684; kernel &#x76F8;&#x540C;) &#x4EE5;&#x53CA;&#x4E00;&#x4E2A; HTTP/S &#x670D;&#x52A1;&#x5668; (Tornado)&#x3002;&#x800C;&#x5BA2;&#x6237;&#x7AEF;&#x5219;&#x662F;&#x4E00;&#x4E2A;&#x6307;&#x5411;&#x670D;&#x52A1;&#x7AEF;&#x5730;&#x5740;&#x7684;&#x6D4F;&#x89C8;&#x5668;&#x9875;&#x9762;&#xFF0C;&#x8D1F;&#x8D23;&#x63A5;&#x53D7;&#x7528;&#x6237;&#x7684;&#x8F93;&#x5165;&#x5E76;&#x8D1F;&#x8D23;&#x6E32;&#x67D3;&#x8F93;&#x51FA;&#x3002;</p>
</blockquote>
<p>&#x5982;&#x4ECA;IPython Notebook&#x5DF2;&#x7ECF;&#x66F4;&#x540D;&#x4E3A;jupyter&#xFF08;&#x8BF4;&#x5B9E;&#x8BDD;&#x611F;&#x89C9;&#x4E0D;&#x5982;&#x539F;&#x6765;&#x7684;&#x540D;&#x5B57;&#xFF09;&#x3002;</p>
<p>&#x6839;&#x636E;Udacity&#x7ED9;&#x51FA;&#x7684;&#x4E0B;&#x8F7D;&#x94FE;&#x63A5;&#x4E0B;&#x8F7D;ipynb&#x6587;&#x4EF6;&#x540E;&#xFF0C;&#x5728;&#x4FDD;&#x5B58;&#x6587;&#x4EF6;&#x7684;&#x8DEF;&#x5F84;&#x4E0B;&#x8F93;&#x5165;&#xFF1A;</p>
<p><code>jupyter notebook Data_Analyst_ND_Project0.ipynb</code></p>
<p>&#x5373;&#x53EF;&#x542F;&#x52A8;&#x672C;&#x5730;&#x670D;&#x52A1;&#x5668;&#xFF0C;&#x5728;&#x6D4F;&#x89C8;&#x5668;&#x4E2D;&#x67E5;&#x770B;&#x548C;&#x7F16;&#x8F91;&#x8BE5;&#x7B14;&#x8BB0;&#x672C;&#x3002;</p>
<h2 id="Project_0:_Chopsticks!">Project 0: Chopsticks!</h2><h3 id="&#x80CC;&#x666F;">&#x80CC;&#x666F;</h3><p>&#x7814;&#x7A76;&#x8005;&#x60F3;&#x8981;&#x627E;&#x51FA;&#x6210;&#x4EBA;&#x548C;&#x513F;&#x7AE5;&#x4F7F;&#x7528;&#x7B77;&#x5B50;&#x7684;&#x6700;&#x9002;&#x5B9C;&#x957F;&#x5EA6;&#x3002;&#x7B77;&#x5B50;&#x4F7F;&#x7528;&#x7684;&#x8868;&#x73B0;&#x7531;&#x5939;&#x8D77;&#x82B1;&#x751F;&#x653E;&#x5230;&#x676F;&#x5B50;&#x91CC;&#x7684;&#x82B1;&#x751F;&#x4E2A;&#x6570;&#x6765;&#x8BC4;&#x4F30;&#x3002;</p>
<p><a href="http://www.ncbi.nlm.nih.gov/pubmed/15676839" target="_blank" rel="external">&#x8FD9;&#x9879;&#x7814;&#x7A76;</a>&#x53D1;&#x8868;&#x5728;1991&#x5E74;&#x7684;Applied Ergonomics&#x4E0A;&#x3002;</p>
<h3 id="&#x5B9E;&#x9A8C;">&#x5B9E;&#x9A8C;</h3><p>&#x5B9E;&#x9A8C;&#x8FDB;&#x884C;&#x4E86;&#x4E24;&#x6B21;&#xFF0C;&#x5206;&#x522B;&#x5BF9;31&#x540D;&#x7537;&#x6027;&#x521D;&#x7EA7;&#x5B66;&#x9662;&#xFF08;&#x76F8;&#x5F53;&#x4E8E;&#x5927;&#x5B66;&#x9884;&#x79D1;&#xFF09;&#x5B66;&#x751F;&#x548C;21&#x540D;&#x5C0F;&#x5B66;&#x751F;&#x8FDB;&#x884C;&#x3002;&#x4F7F;&#x7528;&#x4E86;&#x957F;&#x5EA6;&#x5206;&#x522B;&#x4E3A;180, 210, 240, 270, 300 &#x548C;330 mm&#x7684;&#x7B77;&#x5B50;&#x3002;&#x5B9E;&#x9A8C;&#x91C7;&#x7528;randomised complete block design. </p>
<h3 id="&#x6570;&#x636E;">&#x6570;&#x636E;</h3><p>&#x672C;&#x9879;&#x76EE;&#x53EA;&#x91C7;&#x7528;&#x6210;&#x4EBA;&#x7EC4;&#x7684;&#x6570;&#x636E;&#x3002;&#x6570;&#x636E;&#x5206;&#x4E3A;&#x4E09;&#x5217;&#xFF1A;&#x5939;&#x53D6;&#x6548;&#x7387;&#x3001;&#x5B9E;&#x9A8C;&#x5BF9;&#x8C61;&#x7F16;&#x53F7;&#x3001;&#x7B77;&#x5B50;&#x957F;&#x5EA6;&#x3002;</p>
<p>&#x6BCF;&#x4F4D;&#x6210;&#x4EBA;&#x90FD;&#x4F7F;&#x7528;&#x4E86;&#x4E00;&#x904D;&#x6240;&#x6709;&#x4E0D;&#x540C;&#x957F;&#x5EA6;&#x7684;&#x7B77;&#x5B50;&#x3002;</p>
<h3 id="&#x95EE;&#x9898;">&#x95EE;&#x9898;</h3><ol>
<li>Independent Variable in the Experiment?</li>
<li>Dependent Variable in the Experiment?</li>
<li>Operational Definition of the Dependent Variable?</li>
<li>Controlled Variables? (list at least two)</li>
<li>Best chopstick length?</li>
<li>Relationship between length and efficiency?</li>
<li>Do you agree with the claim made by the researchers and why?</li>
</ol>
<p>&#xFF08;&#x5F85;&#x7EED;&#xFF09;</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>&#x4ECB;&#x7ECD;Udacity&#xFF08;&#x5728;&#x534E;&#x540D;&#x79F0;&#x201C;&#x4F18;&#x8FBE;&#x5B66;&#x57CE;&#x201D;&#xFF09;&#x7684;&#x201C;&#x7EB3;&#x7C73;&#x5B66;&#x4F4D;&#x201D;&#x9879;&#x76EE;&#xFF0C;&#x53CA;&#x5176;&#x6570;&#x636E;&#x5206;&#x6790;&#x5E08;&#x9879;&#x76EE;&#x7684;&#x57FA;&#x672C;&#x60C5;&#x51B5;&#x3002;</p>]]>
    
    </summary>
    
      <category term="Data Analyst" scheme="http://tech.liuxiaozhen.com/tags/Data-Analyst/"/>
    
      <category term="Nanodegree" scheme="http://tech.liuxiaozhen.com/tags/Nanodegree/"/>
    
      <category term="Statistics" scheme="http://tech.liuxiaozhen.com/tags/Statistics/"/>
    
      <category term="Udacity" scheme="http://tech.liuxiaozhen.com/tags/Udacity/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[How to Use Git and GitHub]]></title>
    <link href="http://tech.liuxiaozhen.com/2016/04/06/udacity-GitHub-Notes/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/06/udacity-GitHub-Notes/</id>
    <published>2016-04-05T17:13:45.000Z</published>
    <updated>2016-05-06T08:18:55.000Z</updated>
    <content type="html"><![CDATA[<p>Udacity&#x8BFE;&#x7A0B;&#x4E4B;&#x4E00;&#xFF0C;&#x53EF;&#x80FD;&#x662F;&#x6700;&#x5570;&#x55E6;&#x7684;GitHub&#x6559;&#x7A0B;&#xFF1A;&#xFF09;</p>
<a id="more"></a>
<p><strong>This note is for the Udacity course &#x201C;How to Use Git and Github&#x201D;.</strong></p>
<h2 id="Lesson_1">Lesson 1</h2><h3 id="Find_differences_between_two_large_files">Find differences between two large files</h3><p><strong>Commands</strong>:</p>
<ul>
<li>Windows: FC</li>
<li>Mac/Linux: Diff</li>
</ul>
<blockquote>
<p>Reflect: How did viewing a diff between two versions help you spot the bug?<br>Answer: By looking at the differences between two versions, I know which changes have been made that caused the bug.</p>
</blockquote>
<p>Choose a text editor: Notepad++, Sublim, Atom, emacs, vim, etc. </p>
<p><strong>I picked Atom.</strong></p>
<h3 id="Versions">Versions</h3><ul>
<li>Saving manual copies</li>
<li>Dropbox (periodically save versions automatically)</li>
<li>Google Docs (periodically save versions automatically)</li>
<li>Wikipedia (versions by different authors)</li>
<li>Git</li>
<li>SVN</li>
</ul>
<blockquote>
<p>Reflect: How could having easy access to the entire history of a file make you a more efficient programmer in the long term?<br>I don&#x2019;t have to remember all the changes and the reasons for making them. With all the histories stored in exact forms, I can just look back at them when needed. Also, it helps me to learn from my own mistakes and allow me to do experiments without worrying about breaking things. </p>
</blockquote>
<p><strong>Feature Comparison Chart</strong></p>
<blockquote>
<p>Quiz: When to save: as a programer, when would you want to have version of your code saved?</p>
<ul>
<li>At regular intervals (e.g. every hour)</li>
<li>Whenever a large enough change is made (e.g. 50 lines)</li>
<li>Whenever there is a long pause in editing</li>
<li><strong>When you choose to save a version</strong></li>
</ul>
</blockquote>
<h3 id="Mannual_Commits">Mannual Commits</h3><p>Git requires a message with each commit</p>
<p>Cases that need a new commit:</p>
<ul>
<li>fix off-by-one bug</li>
<li>add cool new feature</li>
<li>improve user docs</li>
</ul>
<h3 id="Use_Git_to_View_History">Use Git to View History</h3><p><strong>[Offline]</strong></p>
<ul>
<li>Each commit has an ID, an Autor, Date/Time, and a message</li>
<li><code>git diff ID1 ID2</code></li>
<li><p>Judgment Call</p>
<blockquote>
<p>Choosing when to commit is a judgment call, and it&#x2019;s not always cut-and-dried. When choosing whether to commit, just keep in mind that each commit should have one clear, logical purpose, and you should never do too much work without committing.</p>
</blockquote>
</li>
</ul>
<h3 id="Commits_with_Multiple_Files">Commits with Multiple Files</h3><ul>
<li>A repository contains multiple files</li>
<li>Make changes in different files together and track in one commit<blockquote>
<p>Reflect: Why do you think some version control systems, like Git, allow saving multiple files in one commit, while others, like Google Docs, treat each file separately?<br>Because Git takes a mannual and logical approach, and keeps the records in one git file for each repo. For others, they keep the records as part of the information related to one file.</p>
</blockquote>
</li>
</ul>
<h3 id="Git_Commands">Git Commands</h3><p><strong>[Online]</strong></p>
<ul>
<li>Clone a repo <code>git clone _repo_url_</code><br>(<a href="https://github.com/udacity/asteroids.git" target="_blank" rel="external">https://github.com/udacity/asteroids.git</a>)</li>
<li><code>git log</code></li>
<li><code>git diff</code></li>
<li>Check commit: <code>git checkout _commit ID_</code></li>
</ul>
<h3 id="Making_Git_configurations">Making Git configurations</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global core<span class="class">.editor</span> <span class="string">&quot;atom --wait&quot;</span></span><br><span class="line">git config --global push<span class="class">.default</span> upstream</span><br><span class="line">git config --global merge<span class="class">.conflictstyle</span> diff3</span><br></pre></td></tr></table></figure>
<h3 id="Summary">Summary</h3><ul>
<li>Why use Git</li>
<li>Git Setup</li>
<li>Git commands: clone, log, diff, checkout</li>
</ul>
<h2 id="Lesson_2">Lesson 2</h2><h3 id="Initialize">Initialize</h3><p><strong>[Offline]</strong></p>
<ul>
<li><code>git init</code></li>
<li>When a git is initialized, no commit is included</li>
<li>Check git status: <code>git status</code></li>
</ul>
<h3 id="Choosing_what_changes_to_commit">Choosing what changes to commit</h3><ul>
<li>working directory -&gt; staging area -&gt; repository</li>
<li><code>git add _file_to_update_</code> will add named files to staging area</li>
<li>remove from staging area by using <code>git reset</code></li>
</ul>
<h3 id="Write_a_commit_message">Write a commit message</h3><ul>
<li><code>git commit</code> will open the editor</li>
<li>standard practice: write a message as if it is a command</li>
<li><code>git commit -m &quot;Commit message&quot;</code></li>
<li>commit message <a href="http://udacity.github.io/git-styleguide/" target="_blank" rel="external">style guide</a></li>
</ul>
<h3 id="Git_diff_revisited">Git diff revisited</h3><ul>
<li><code>git diff</code> will compare the <strong>working directory</strong> with the <strong>staging area</strong></li>
<li><code>git diff --staged</code> will compare the <strong>staging area</strong> with the <strong>repository</strong></li>
<li>_Be careful! <code>git reset --hard</code> is not reversable_</li>
<li>Leave &#x2018;detached HEAD&#x2019; state: <code>git checkout master</code></li>
</ul>
<h3 id="Create_and_commit_branches">Create and commit branches</h3><ul>
<li>One branch is enough: fix bug, new feature, update docs</li>
<li>More than one branch: experimental feature, Italian version</li>
<li>Most recent commit of a branch: tip of a branch</li>
<li>Show all the branches and the current branch is marked with * : <code>git branch</code></li>
<li>Create a branch: <code>git branch &quot;branch_name&quot;</code></li>
<li>Switch to a branch: <code>git checkout &quot;branch_name&quot;</code></li>
</ul>
<h3 id="Branches_for_Collaboration">Branches for Collaboration</h3><p><strong>[Online]</strong></p>
<ul>
<li>remote branch</li>
<li>branch has parents &#x201C;reachability&#x201D;</li>
<li>checkout a commit as a new branch: <code>git checkout -b _new_branch_name_</code></li>
</ul>
<h3 id="Combining_Simple_Files">Combining Simple Files</h3><ul>
<li>merged branch has both parents</li>
<li><code>git merge branch_1 branch_2</code></li>
<li><code>git show</code> show the diff of a commit with its parent</li>
<li>deleting branches: <code>git branch -d _branch_name_</code></li>
<li>Merge conflict: when both branches modify the same part of the files</li>
<li>Commit the conflict resolution</li>
<li><code>git log -n 1</code> show only 1 commit log</li>
</ul>
<h3 id="Summary-1">Summary</h3><ul>
<li>Initialize git</li>
<li>Create commits</li>
<li>Create and commit branches</li>
<li>Merge branches</li>
</ul>
<h2 id="Lesson_3">Lesson 3</h2><h3 id="Sync_with_repositories_on_GitHub">Sync with repositories on GitHub</h3><ul>
<li>Remote repo</li>
<li>push and pull</li>
<li>no working directory or staging area on GitHub side</li>
</ul>
<h3 id="Forking_a_Repository">Forking a Repository</h3><ul>
<li>click fork button</li>
<li>= create a clone on GitHub instead of on your local computer.</li>
<li>Can add collaborators</li>
<li><code>git remote -v</code></li>
<li>push</li>
</ul>
<h3 id="Merge_the_Changes_Together">Merge the Changes Together</h3><ul>
<li>Fast-forward merge</li>
<li><code>git merge master origin/master</code></li>
<li>resolve conflict</li>
</ul>
<h3 id="Pull_request">Pull request</h3><ul>
<li>Ask for your pull request to be merged</li>
<li>after merging, delete branch</li>
</ul>
<h3 id="Summary-2">Summary</h3><ul>
<li>Clone and Fork</li>
<li>Merge and Pull request</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>Udacity&#x8BFE;&#x7A0B;&#x4E4B;&#x4E00;&#xFF0C;&#x53EF;&#x80FD;&#x662F;&#x6700;&#x5570;&#x55E6;&#x7684;GitHub&#x6559;&#x7A0B;&#xFF1A;&#xFF09;</p>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[Machine Learning by Stanford University Week 2]]></title>
    <link href="http://tech.liuxiaozhen.com/2016/04/05/Machine-learning-W2/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/05/Machine-learning-W2/</id>
    <published>2016-04-05T07:00:00.000Z</published>
    <updated>2016-05-06T08:15:43.000Z</updated>
    <content type="html"><![CDATA[<p>Andrew Ng&#x5728;Coursera&#x4E0A;&#x5F00;&#x8BBE;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x8BFE;&#x7A0B;&#x7B2C;&#x4E8C;&#x5468;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x4ECB;&#x7ECD;&#x4E86;&#x591A;&#x5143;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x3002;</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course &#x201C;Machine Learning&#x201D; taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="Environment_Setup">Environment Setup</h2><p>Octave and MATLAB are preferred in machine learning. </p>
<p>For more information about Octave and MATLAB, see:<br><a href="https://www.coursera.org/learn/machine-learning/supplement/Mlf3e/more-octave-matlab-resources" target="_blank" rel="external">https://www.coursera.org/learn/machine-learning/supplement/Mlf3e/more-octave-matlab-resources</a></p>
<h2 id="Multivariate_Linear_Regression">Multivariate Linear Regression</h2><h3 id="Multiple_Features">Multiple Features</h3><p>&#x4E3A;&#x4EC0;&#x4E48;&#x9700;&#x8981;multiple features? &#x5BF9;&#x4E8E;&#x8BB8;&#x591A;&#x95EE;&#x9898;&#xFF0C;&#x5F71;&#x54CD;&#x9884;&#x6D4B;&#x7ED3;&#x679C;&#x7684;&#x5E76;&#x4E0D;&#x6B62;&#x4E00;&#x4E2A;&#x56E0;&#x7D20;&#xFF0C;&#x56E0;&#x6B64;&#xFF0C;&#x9700;&#x8981;&#x591A;&#x4E2A;&#x53D8;&#x91CF;&#x6765;&#x53CD;&#x6620;&#x4E0D;&#x540C;&#x7684;&#x5F71;&#x54CD;&#x56E0;&#x7D20;&#x3002;</p>
<p>Multiple Features&#x5982;&#x4F55;&#x7528;&#x7EBF;&#x6027;&#x65B9;&#x7A0B;&#x5F0F;&#x6765;&#x8868;&#x793A;&#xFF1F;</p>
<p>$h_\theta(x) = \theta_0 +\theta_1x_1+\theta_2x_2+&#x2026;+\theta_nx_n = \theta^TX$</p>
<h3 id="Cost_Function_for_Multiple_Variables">Cost Function for Multiple Variables</h3><p>$\theta$ is an $n+1$ -dimention vector</p>
<p>$J(\theta_0,\theta_1,&#x2026;,\theta_n)=\frac1{2m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2$</p>
<h3 id="Gradient_Descent_for_Multiple_Variables">Gradient Descent for Multiple Variables</h3><p>Repeat{</p>
<p>$\theta_j := \theta_j - \alpha\frac1m\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}$</p>
<p>(simutaneously update $\theta_j$ for $j = 0,&#x2026;,n$</p>
<p>notice that $x_0 = 1$)</p>
<p>}</p>
<h3 id="Feature_Scaling">Feature Scaling</h3><ul>
<li><p>Idea: get every feature into approximately a $-1\leq {x_i}\leq 1$ range</p>
</li>
<li><p>Mean normalization: replace $x_i$ with $x_i - \mu_i$ to make features have approximately zero mean (Do not apply to $x_0 = 1$)</p>
</li>
</ul>
<h3 id="Learning_Rate_$\alpha$">Learning Rate $\alpha$</h3><ul>
<li>If $\alpha$ is too small: slow convergence.</li>
<li>If $\alpha$ is too large: $J(\theta)$ may not decrease on every iteration; may not converge. (slow converge also possible)</li>
<li>To choose $\alpha$, try: &#x2026;,0.001, 0.01, 0.1, 1,&#x2026;</li>
</ul>
<h3 id="Features_and_Polynomial_Regression">Features and Polynomial Regression</h3><ul>
<li><p>Polynomial regression example:    $\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3$; let $x_1 = x, x_2 = x^2, x_3 = x^3$</p>
</li>
<li><p>Other possiblities: $\theta_0+\theta_1x+\theta_2\sqrt{x}$, let $x_1 = x, x_2 = \sqrt{x}$</p>
</li>
</ul>
<h2 id="Computing_Parameters_Analytically">Computing Parameters Analytically</h2><p>Suppose there are $m$ examples; $n$ features</p>
<p>$\theta = (X^TX)^{-1}X^Ty$<br>$(X^TX)^{-1}$ is inverse of matrix $X^TX$</p>
<p>if $m &lt; n$,<br>$X^TX$ may be non-inversible. </p>
<p>Therefore use <code>pinv</code> in Octave:<br><code>pinv(X&apos;*X)*X&apos;*y</code></p>
<table>
<thead>
<tr>
<th>Gradient Discent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose $\alpha$</td>
<td>No need to choose $\alpha$</td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>Don&#x2019;t need to iterate</td>
</tr>
<tr>
<td>Works well even when $n$ is large</td>
<td>Need to compute $(X^TX)^{-1}$ ($O(n^3)$), slow if $n$ is very large</td>
</tr>
</tbody>
</table>
<p>Therefore, if n is less than 1000, use normal equation. Otherwise, use gradient descent.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Andrew Ng&#x5728;Coursera&#x4E0A;&#x5F00;&#x8BBE;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x8BFE;&#x7A0B;&#x7B2C;&#x4E8C;&#x5468;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x4ECB;&#x7ECD;&#x4E86;&#x591A;&#x5143;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x3002;</p>]]>
    
    </summary>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Linear Regression" scheme="http://tech.liuxiaozhen.com/tags/Linear-Regression/"/>
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Machine Learning by Stanford University Week 1]]></title>
    <link href="http://tech.liuxiaozhen.com/2016/03/10/Machine-learning-W1/"/>
    <id>http://tech.liuxiaozhen.com/2016/03/10/Machine-learning-W1/</id>
    <published>2016-03-10T12:41:19.000Z</published>
    <updated>2016-05-06T08:16:05.000Z</updated>
    <content type="html"><![CDATA[<p>Andrew Ng&#x5728;Coursera&#x4E0A;&#x5F00;&#x8BBE;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x8BFE;&#x7A0B;&#x7B2C;&#x4E00;&#x5468;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x4ECB;&#x7ECD;&#x4E86;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x57FA;&#x672C;&#x5B9A;&#x4E49;&#xFF0C;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x53CA;&#x65E0;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#xFF0C;&#x4EE5;&#x53CA;&#x5355;&#x53D8;&#x91CF;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x3002;</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course &#x201C;Machine Learning&#x201D; taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="Introduction">Introduction</h2><p>Definition of Machine Learning (Tom Mitchell): </p>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>
<h3 id="Supervised_Learning">Supervised Learning</h3><ul>
<li>Correct outputs are known. </li>
<li>Categorized into <strong>regression</strong> and <strong>classification</strong> problems</li>
<li>Regression problem: predict results within a continuous output</li>
<li>Classification problem: predict results in a discrete output (0 or 1)</li>
</ul>
<p>Example: </p>
<ul>
<li>Boston Housing price vs size (regression)</li>
<li>Breast cancer malignant vs benign (classification)<ul>
<li>more than one attribute</li>
</ul>
</li>
</ul>
<h3 id="Unsupervised_Learning">Unsupervised Learning</h3><ul>
<li>No known correct output (feedback)</li>
<li>Derive data structure by clustering the data based on relationships among the variables in the data</li>
<li>Examples: google story collection; cocktail party (distiguish two voices)</li>
</ul>
<h2 id="Linear_Regression_with_One_Variable">Linear Regression with One Variable</h2><h3 id="Model_Representation">Model Representation</h3><p>Training set of housing prices (Portland, OR)<br>Notation:<br>$m$ = Number of training examples<br>$x$&#x2019;s = &#x201C;input&#x201D; variable / features<br>$y$&#x2019;s = &#x201C;output&#x201D; variable / &#x201C;target&#x201D; variable</p>
<p>$(x,y)$ - one training example</p>
<p>$(x^{(i)},y^{(i)})$ - $i^{th}$ training example</p>
<p><img src="/2016/03/10/Machine-learning-W1/Coursera-ml-w1-01.png" alt=""></p>
<p>How do we represent $h$?</p>
<p>  $$h_{\theta}(x) = \theta_0+\theta_1x$$</p>
<h3 id="Cost_Function">Cost Function</h3><p><strong>Idea</strong>: </p>
<p>Choose ${\theta_0}$, ${\theta_1}$ so that $h_{\theta}(x)$ is close to $y$ for our training examples $(x,y)$</p>
<h3 id="Cost_Function_-_Intuition">Cost Function - Intuition</h3><p>Hypothesis: $$h_{\theta}(x) = \theta_0+\theta_1x$$</p>
<p>Parameters: $\theta_0, \theta_1$</p>
<p>Cost Function:</p>
<p>$$J (\theta_0,\theta_1) = \frac 1{2m}{\sum_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2} $$</p>
<p>Goal:<br> $minimize\  J(\theta_0,\theta_1)$</p>
<p><img src="/2016/03/10/Machine-learning-W1/Coursera-ml-w1-02.png" alt=""></p>
<h3 id="Gradient_Descent">Gradient Descent</h3><p>Have some function $J(\theta_0,\theta_1)$</p>
<p>want $min \ J(\theta_0,\theta_1)$</p>
<p>Outline:</p>
<ul>
<li>start with some $\theta_0,\theta_1$</li>
<li>keep changing $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$ until we hopefully end up at a minimum</li>
</ul>
<h3 id="Gradient_descent_algorithm">Gradient descent algorithm</h3><p>repeat until convergence {</p>
<p>$$\theta_j := \theta_j - \alpha \frac{\partial} {\partial \theta_j} J(\theta_0,\theta_1) $$     ( for $j = 0$ and $j =1$)</p>
<p>}</p>
<p><strong>Correct: simultaneous update $\theta_0$ and $\theta_1$</strong></p>
<p>$\alpha$ is the learning rate. </p>
<ul>
<li><p>if $\alpha$ is too small, gradient descent can be slow. </p>
</li>
<li><p>if $\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. </p>
</li>
</ul>
<p>Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed. </p>
<h3 id="Gradient_Descent_for_Linear_Regression">Gradient Descent for Linear Regression</h3><p>repeat until convergence {</p>
<p>$$\theta_0 := \theta_0 - \alpha \frac1m \sum_{i=1}^m \left( h_\theta (x^{(i)})-y^{(i)}\right) $$</p>
<p>$$\theta_1 := \theta_1 - \alpha \frac1m \sum_{i=1}^m \left( h_\theta (x^{(i)})-y^{(i)} \right)\cdot x^{(i)} $$<br>}</p>
<h4 id="&#x201C;Batch&#x201D;_Gradient_Descent">&#x201C;Batch&#x201D; Gradient Descent</h4><p>Each step of gradient descent uses all the training examples</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Andrew Ng&#x5728;Coursera&#x4E0A;&#x5F00;&#x8BBE;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x8BFE;&#x7A0B;&#x7B2C;&#x4E00;&#x5468;&#x7684;&#x5185;&#x5BB9;&#xFF0C;&#x4ECB;&#x7ECD;&#x4E86;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x57FA;&#x672C;&#x5B9A;&#x4E49;&#xFF0C;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x53CA;&#x65E0;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#xFF0C;&#x4EE5;&#x53CA;&#x5355;&#x53D8;&#x91CF;&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x3002;</p>]]>
    
    </summary>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Linear Regression" scheme="http://tech.liuxiaozhen.com/tags/Linear-Regression/"/>
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python for Everybody by University of Michigan (1)]]></title>
    <link href="http://tech.liuxiaozhen.com/2015/12/06/Python-for-Everybody-1/"/>
    <id>http://tech.liuxiaozhen.com/2015/12/06/Python-for-Everybody-1/</id>
    <published>2015-12-06T03:51:47.000Z</published>
    <updated>2016-05-06T08:09:53.000Z</updated>
    <content type="html"><![CDATA[<p>&#x4ECB;&#x7ECD;&#x4E86;&#x7F16;&#x7A0B;&#x57FA;&#x672C;&#x77E5;&#x8BC6;&#xFF0C;Python&#x5B89;&#x88C5;&#xFF0C;&#x8868;&#x8FBE;&#x5F0F;&#xFF0C;&#x53D8;&#x91CF;&#x7B49;&#x3002;</p>
<a id="more"></a>
<p><strong>This note is for the first course of University of Michigan online series &#x201C;Python for Everybody Specialization&#x201D; taught by Professor Charles Severance on Coursera.org, 2015 October session.</strong></p>
<h1 id="Week_1">Week 1</h1><h2 id="Why_Program?">Why Program?</h2><h2 id="Hardware">Hardware</h2><ul>
<li>CPU</li>
<li>I/O</li>
<li>Main sMemory (RAM)</li>
<li>Secondary Memory</li>
</ul>
<h2 id="Python">Python</h2><ul>
<li>Vocabulary/Words - Variables and Reserved words</li>
<li>Sentence structure - valid syntax patterns</li>
<li>Story structure - constructing a program for a purpose</li>
</ul>
<h1 id="Week_2">Week 2</h1><h2 id="Install_Python">Install Python</h2><ul>
<li>IDLE</li>
<li>Python Downloads: <a href="https://www.python.org/downloads/" target="_blank" rel="external">https://www.python.org/downloads/</a></li>
<li>Text Editor</li>
</ul>
<h1 id="Week_3">Week 3</h1><h2 id="Simple_program">Simple program</h2><ul>
<li>Sequential steps</li>
<li>Conditional steps</li>
<li>Repeated steps</li>
</ul>
<h2 id="Think_like_a_program">Think like a program</h2><ul>
<li>How to find the largest number?</li>
</ul>
<h1 id="Week_4">Week 4</h1><h2 id="Expressions">Expressions</h2><ul>
<li>Variables and Constants <ul>
<li>Constants</li>
<li>Python variable name rules: start with letters, case sensitive</li>
</ul>
</li>
<li>Sentences or lines<ul>
<li>Assignment statement: <code>x = 2</code></li>
<li>Assignment with expression: <code>x = x + 2</code> <ul>
<li>Print statement: <code>print x</code></li>
</ul>
</li>
</ul>
</li>
<li>Operator<ul>
<li>Numeric expressions: +, -, _, /, *_, %</li>
<li>Operator precedence rules: parenthesis, power, multiplication, addition, left to right<ul>
<li>Reserved words</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Variables">Variables</h2><ul>
<li>Type<ul>
<li>Interger</li>
<li>Float</li>
<li>String</li>
<li>&#x2026;</li>
</ul>
</li>
<li>Type conversions<ul>
<li>Type of x: <code>type(x)</code></li>
<li>Convert x to an interger <code>int(x)</code></li>
<li>Convert x to a float<code>float(x)</code></li>
</ul>
</li>
<li>User Input<ul>
<li><code>nam = raw_input(&quot;Who are you?&quot;)</code></li>
</ul>
</li>
<li>Comments in Python: anything after <code>#</code></li>
<li>Variable naming style</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>&#x4ECB;&#x7ECD;&#x4E86;&#x7F16;&#x7A0B;&#x57FA;&#x672C;&#x77E5;&#x8BC6;&#xFF0C;Python&#x5B89;&#x88C5;&#xFF0C;&#x8868;&#x8FBE;&#x5F0F;&#xFF0C;&#x53D8;&#x91CF;&#x7B49;&#x3002;</p>]]>
    
    </summary>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Python" scheme="http://tech.liuxiaozhen.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[启]]></title>
    <link href="http://tech.liuxiaozhen.com/2015/04/03/start/"/>
    <id>http://tech.liuxiaozhen.com/2015/04/03/start/</id>
    <published>2015-04-03T09:17:05.000Z</published>
    <updated>2016-03-12T07:08:19.000Z</updated>
    <content type="html"><![CDATA[<p>&#x4ECE;&#x5C0F;&#x5B66;&#x5C31;&#x5F00;&#x59CB;&#x7F16;&#x7A0B;&#xFF0C;&#x4F46;&#x662F;&#x4EC5;&#x9650;&#x4E8E;&#x597D;&#x73A9;&#xFF1B;&#x5728;&#x5B66;&#x4E60;&#x79D1;&#x7814;&#x4E2D;&#x5076;&#x5C14;&#x6D89;&#x53CA;&#x7F16;&#x7A0B;&#xFF0C;&#x4E5F;&#x4E0D;&#x8FC7;&#x662F;&#x7B80;&#x77ED;&#x7684;&#x51E0;&#x5341;&#x884C;&#x3002;&#x4ECE;2009&#x5E74;&#x5165;&#x624B;iPod Touch&#x81F3;&#x4ECA;&#xFF0C;&#x5173;&#x4E8E;App&#x7684;&#x60F3;&#x6CD5;&#x65E0;&#x6570;&#xFF0C;&#x5374;&#x59CB;&#x7EC8;&#x6CA1;&#x6709;&#x6210;&#x4E3A;&#x4E00;&#x4E2A;&#x771F;&#x6B63;&#x7684;&#x521B;&#x9020;&#x8005;&#x3002;</p>
<p>tinyfool&#x5728;&#x53BB;&#x5E74;&#x7684;&#x4E00;&#x7BC7;<a href="http://www.jianshu.com/p/c27906bb8061" target="_blank" rel="external">&#x300A;&#x5BFB;&#x627E;&#x548C;&#x7A81;&#x7834;&#x5FC3;&#x969C;&#x300B;</a>&#x4E2D;&#x8BF4;&#xFF1A;</p>
<blockquote>
<p>&#x4EBA;&#x751F;&#x4E2D;&#x603B;&#x6709;&#x4E9B;&#x969C;&#x788D;&#x963B;&#x6321;&#x7740;&#x4F60;&#x53BB;&#x505A;&#x4F60;&#x60F3;&#x505A;&#x7684;&#x4E8B;&#x60C5;&#xFF0C;&#x4F46;&#x662F;&#x8FD9;&#x4E9B;&#x969C;&#x788D;&#x91CC;&#x9762;&#x6709;&#x4E00;&#x4E9B;&#x662F;&#x7269;&#x7406;&#x969C;&#x788D;&#xFF0C;&#x5982;&#x679C;&#x4F60;&#x771F;&#x7684;&#x6CA1;&#x6709;&#x65F6;&#x95F4;&#x548C;&#x91D1;&#x94B1;&#xFF0C;&#x4E0D;&#x53BB;&#x65C5;&#x6E38;&#x4E5F;&#x5C31;&#x4E0D;&#x53BB;&#x65C5;&#x6E38;&#x4E86;&#x3002;&#x5982;&#x679C;&#x4F60;&#x6709;&#x65F6;&#x95F4;&#x3001;&#x6709;&#x91D1;&#x94B1;&#xFF0C;&#x4E5F;&#x6709;&#x4E00;&#x4E2A;&#x8BF4;&#x8D70;&#x5C31;&#x8D70;&#x7684;&#x5FC3;&#xFF0C;&#x4F46;&#x662F;&#x4F60;&#x54EA;&#x91CC;&#x90FD;&#x6CA1;&#x6709;&#x53BB;&#x8FC7;&#xFF0C;&#x90A3;&#x5C31;&#x662F;&#x5FC3;&#x969C;&#x3002;</p>
</blockquote>
<p>&#x5173;&#x4E8E;&#x7F16;&#x7A0B;&#xFF0C;&#x6211;&#x60F3;&#x6211;&#x5E76;&#x975E;&#x771F;&#x7684;&#x5B8C;&#x5168;&#x6CA1;&#x6709;&#x65F6;&#x95F4;&#xFF0C;&#x800C;&#x4EC5;&#x4EC5;&#x51FA;&#x4E8E;&#x5FC3;&#x969C;&#x800C;&#x6CA1;&#x6709;&#x505A;&#x3002;&#x8FD1;&#x5E74;&#x6765;&#xFF0C;&#x5728;&#x6548;&#x7387;&#x548C;&#x5FC3;&#x667A;&#x65B9;&#x9762;&#x6536;&#x83B7;&#x8BB8;&#x591A;&#xFF0C;&#x5BF9;&#x4E8E;&#x5F00;&#x542F;&#x8FD9;&#x6837;&#x4E00;&#x6BB5;&#x65C5;&#x7A0B;&#x6162;&#x6162;&#x6709;&#x4E86;&#x4FE1;&#x5FC3;&#x3002;&#x5199;&#x7B14;&#x8BB0;&#xFF0C;&#x66F4;&#x591A;&#x7684;&#x662F;&#x5E0C;&#x671B;&#x9760;&#x8F93;&#x51FA;&#x7684;&#x8FC7;&#x7A0B;&#x6765;&#x52A0;&#x5F3A;&#x5B66;&#x4E60;&#x6548;&#x679C;&#x3002;&#x82E5;&#x80FD;&#x5728;&#x6BD4;&#x7279;&#x4E16;&#x754C;&#xFF0C;&#x65E0;&#x610F;&#x95F4;&#x7ED9;&#x4E00;&#x4E9B;&#x4EBA;&#x5E2E;&#x52A9;&#xFF0C;&#x5219;&#x5E78;&#x751A;&#x81F3;&#x54C9;&#x3002;</p>
<p><img src="/2015/04/03/start/dune.jpeg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>&#x4ECE;&#x5C0F;&#x5B66;&#x5C31;&#x5F00;&#x59CB;&#x7F16;&#x7A0B;&#xFF0C;&#x4F46;&#x662F;&#x4EC5;&#x9650;&#x4E8E;&#x597D;&#x73A9;&#xFF1B;&]]>
    </summary>
    
  </entry>
  
</feed>