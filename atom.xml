<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>刘虓震的技术笔记</title>
  <subtitle>Live in the Future, then Build What&#39;s Missing</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tech.liuxiaozhen.com/"/>
  <updated>2017-02-24T05:06:46.000Z</updated>
  <id>http://tech.liuxiaozhen.com/</id>
  
  <author>
    <name>Liu Xiaozhen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>斯坦福计算机系本科生人工智能方向课程资源</title>
    <link href="http://tech.liuxiaozhen.com/2017/02/24/Stanford-AI-Curriculum/"/>
    <id>http://tech.liuxiaozhen.com/2017/02/24/Stanford-AI-Curriculum/</id>
    <published>2017-02-24T02:25:17.000Z</published>
    <updated>2017-02-24T05:06:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>Version 1.2 By <a href="http://tech.liuxiaozhen.com">Liu Xiaozhen</a> Updated on 2017/2/23</p>
<p>2016年8月我研究了一下斯坦福计算机系本科生人工智能方向的课程设置，最近整理出了相关资源作了汇总。本文侧重自然语言处理方向，遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh" target="_blank" rel="external">CC BY-NC-SA 3.0</a> 协议。若有意补充完善其他课程信息或报告错误，欢迎<a href="https://www.zybuluo.com/xiaozhenliu/note/665756" target="_blank" rel="external">在此</a>批注。</p>
<h2 id="毕业要求"><a href="#毕业要求" class="headerlink" title="毕业要求"></a>毕业要求</h2><p><a href="http://csmajor.stanford.edu/ProgramSheets/CS_AI_1617PS.pdf" target="_blank" rel="external">2016-2017 AI Track Program Sheet</a></p>
<h2 id="课程解读"><a href="#课程解读" class="headerlink" title="课程解读"></a>课程解读</h2><h3 id="基础课"><a href="#基础课" class="headerlink" title="基础课"></a>基础课</h3><ul>
<li><p>数学 (26学分＋)： 微积分(MATH41 &amp; 42)、计算数学基础(CS103)、计算机科学家的概率导论(CS109)、两门选修</p>
</li>
<li><p>科学（11学分＋）：力学、电磁学、一门选修</p>
</li>
<li><p>社会中的科技（略）</p>
</li>
<li><p>工程基础(13学分＋)：抽象编程(CS106B/X)、电子学导论(ENGR40)、一门选修</p>
</li>
</ul>
<h3 id="核心课程-（15学分）"><a href="#核心课程-（15学分）" class="headerlink" title="核心课程 （15学分）"></a>核心课程 （15学分）</h3><ul>
<li>计算机组成与系统 CS107/107E </li>
<li>计算机系统原理 CS110</li>
<li>算法设计与分析 CS161</li>
</ul>
<h3 id="AI-Track-进阶课程（25学分，至少7门）"><a href="#AI-Track-进阶课程（25学分，至少7门）" class="headerlink" title="AI Track 进阶课程（25学分，至少7门）"></a>AI Track 进阶课程（25学分，至少7门）</h3><ul>
<li>人工智能原理及技术 CS221</li>
<li>Track B 课程中选两门：CS223A, 224M, 224N, 228,229, 131/231A</li>
<li>Track B 课程中再选一门，或Track C课程中选一门（略）</li>
<li>三门选修：Track B/C 或其他计算机系课程中选</li>
</ul>
<h2 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h2><p>本列表去除了与AI不相关的通识课程和最基础的微积分等，选修部分是自己研究了可选课程后选择的。总的来说，如果你把这些课全学完，就达到了专业课程部分的要求。Stanford可选的课程很多，在<a href="http://csmajor.stanford.edu/ProgramSheets/CS_AI_1617PS.pdf" target="_blank" rel="external">Program Sheet</a>中已有列表，恕不一一列出。</p>
<h3 id="基础与核心课程"><a href="#基础与核心课程" class="headerlink" title="基础与核心课程"></a>基础与核心课程</h3><p>部分无视频课程添加了来自Coursera, edx 或 MIT OCW 对应课程的链接。</p>
<ul>
<li>CS103 计算数学基础: <a href="http://web.stanford.edu/class/cs103/" target="_blank" rel="external">最新课程</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/" target="_blank" rel="external">MIT课程</a>(含视频)</li>
<li>CS106B 抽象编程: <a href="https://see.stanford.edu/Course/CS106B" target="_blank" rel="external">课堂录制</a>, <a href="http://web.stanford.edu/class/cs106b/" target="_blank" rel="external">最新课程</a></li>
<li>CS107 计算机组成: <a href="http://web.stanford.edu/class/cs107/" target="_blank" rel="external">最新课程</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-004-computation-structures-spring-2009/calendar/" target="_blank" rel="external">MIT课程</a>, <a href="https://www.edx.org/course/computation-structures-part-1-digital-mitx-6-004-1x-0" target="_blank" rel="external">MOOC on edX</a></li>
<li>CS109: 计算机科学家的概率导论: <a href="https://web.stanford.edu/class/cs109/" target="_blank" rel="external">最新课程</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/" target="_blank" rel="external">MIT课程</a>, <a href="https://www.edx.org/course/introduction-probability-science-mitx-6-041x-2?utm_source=OCW&amp;utm_medium=CHP&amp;utm_campaign=OCW" target="_blank" rel="external">MOOC on edX</a></li>
<li>CS161 算法设计与分析: <a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=IntroToAlgorithms" target="_blank" rel="external">课堂录制</a>, <a href="http://web.stanford.edu/class/cs161/" target="_blank" rel="external">最新课程</a>, <a href="https://www.coursera.org/specializations/algorithms" target="_blank" rel="external">MOOC on Coursera</a></li>
<li>CS110: 计算机系统原理: <a href="http://web.stanford.edu/class/cs110/winter-2017/" target="_blank" rel="external">最新课程</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-033-computer-system-engineering-spring-2009/" target="_blank" rel="external">MIT课程</a>(含视频)</li>
<li>EE263: 线性动态系统导论（选修）：<a href="https://see.stanford.edu/Course/EE263" target="_blank" rel="external">课堂录制</a>, <a href="http://ee263.stanford.edu/" target="_blank" rel="external">最新课程</a> （注，此处原为MATH113, 根据Quora上Stanford的学生推荐，这门课对于AI Track更有帮助）</li>
<li>CS157: 逻辑与自动推理（选修）：<a href="http://logic.stanford.edu/classes/cs157/current/" target="_blank" rel="external">最新课程</a>, <a href="https://www.coursera.org/learn/logic-introduction" target="_blank" rel="external">MOOC on Coursera</a></li>
<li>ENGR 62 最优化导论（选修）：<a href="http://web.stanford.edu/class/engr62/" target="_blank" rel="external">最新课程</a>, <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-093j-optimization-methods-fall-2009/" target="_blank" rel="external">MIT课程</a></li>
</ul>
<h3 id="AI-Track-进阶课程（自然语言处理相关）"><a href="#AI-Track-进阶课程（自然语言处理相关）" class="headerlink" title="AI Track 进阶课程（自然语言处理相关）"></a>AI Track 进阶课程（自然语言处理相关）</h3><ul>
<li>CS221: 人工智能原理及技术：<a href="http://web.stanford.edu/class/cs221/" target="_blank" rel="external">最新课程</a></li>
<li>CS224N: 自然语言处理（选修Track B）<a href="https://see.stanford.edu/Course/CS224N" target="_blank" rel="external">课堂录制</a>, <a href="http://cs224n.stanford.edu" target="_blank" rel="external">最新课程</a></li>
<li>CS228: 概率图模型（选修Track B）: <a href="http://cs228.stanford.edu/" target="_blank" rel="external">最新课程</a>, <a href="https://www.coursera.org/specializations/probabilistic-graphical-models" target="_blank" rel="external">MOOC on Coursera</a></li>
<li>CS229: 机器学习（选修Track B）:<a href="https://see.stanford.edu/Course/CS229" target="_blank" rel="external">课堂录制</a>, <a href="http://cs229.stanford.edu/" target="_blank" rel="external">最新课程</a></li>
<li>CS224U: 自然语言理解（选修）: <a href="https://www.youtube.com/playlist?list=PLfmUaIBTH8exY7fZnJss508Bp8k1R8ASG" target="_blank" rel="external">课堂录制</a>（部分）, <a href="http://web.stanford.edu/class/cs224u/" target="_blank" rel="external">最新课程</a></li>
<li>EE376A: 信息论（选修）: <a href="http://web.stanford.edu/class/ee376a/" target="_blank" rel="external">最新课程</a>, <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-050j-information-and-entropy-spring-2008/" target="_blank" rel="external">MIT课程</a>(含视频)</li>
<li>CS124/LING180: 从语言到信息（选修）<a href="http://web.stanford.edu/class/cs124/" target="_blank" rel="external">最新课程</a></li>
</ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Version 1.2 By &lt;a href=&quot;http://tech.liuxiaozhen.com&quot;&gt;Liu Xiaozhen&lt;/a&gt; Updated on 2017/2/23&lt;/p&gt;
&lt;p&gt;2016年8月我研究了一下斯坦福计算机系本科生人工智能方向的课程设置，最近整理
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>人工智能课程(0)：简介与课程大纲</title>
    <link href="http://tech.liuxiaozhen.com/2017/02/17/Udacity-AIND-00/"/>
    <id>http://tech.liuxiaozhen.com/2017/02/17/Udacity-AIND-00/</id>
    <published>2017-02-17T13:57:08.000Z</published>
    <updated>2017-02-25T06:29:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.udacity.com/ai" target="_blank" rel="external">Artificial Intelligence Nanodegree (AIND) </a>是优达学城近期新推出的纳米学位课程。与<a href="https://www.udacity.com/drive" target="_blank" rel="external">自动驾驶</a>和<a href="https://www.udacity.com/degrees/vr-developer-nanodegree--nd017" target="_blank" rel="external">虚拟现实</a>纳米学位一样，AIND需要学员在固定的时限内完成项目（6个月内），并且要求每周付出比原先的传统纳米学位（如前端和全栈工程师，数据分析和机器学习等）更多的时间（每周15小时）。在招聘方面，AIND也只给按时完成所有项目顺利结业的学生推荐。</p>
<p>AIND加入了许多社区支持，比如，由于必须在同样时间安排下完成任务，同步学习的同学多了许多；又比如加入了“mentor”这一角色，随时给学生各个方面的指导。不过，这些支持的质量如何，还有待观察。</p>
<p>AIND分为两个学期，各三个月。学期1是基础课程，学期2则是在三个具体方向中选择（机器视觉、语音识别、自然语言处理）。如果你愿意多花几倍的钱，也可以把三个方向都学了（但不是同时）。</p>
<p>总的来说，纳米学位越来越像传统大学的设置了，除了教室还是在线的，入学也没有硬门槛。关于在线学习的效果和受认可的程度早就已经被讨论了无数遍。至今，Udacity仍然是走得最远的一个。未来不是预测出来的，而是创造出来的。看看这个课程里有些什么吧。</p>
<h1 id="Term-1-课程大纲"><a href="#Term-1-课程大纲" class="headerlink" title="Term 1 课程大纲"></a>Term 1 课程大纲</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>Lesson 1: Welcome to the AI Nanodegree</li>
<li>Lesson 2: Setting up with Anaconda</li>
<li>Lesson 3: Solving a Sudoku with AI (<em>Project DUE on Feb 24</em>)</li>
<li>Lesson 4:  Introduction to AI<h2 id="Search-and-Optimization"><a href="#Search-and-Optimization" class="headerlink" title="Search and Optimization"></a>Search and Optimization</h2></li>
<li>Lesson 5: Intruduction to Game Playing</li>
<li>Lesson 6: Advanced Game Playing (<em>Project DUE on Mar 17</em>)</li>
<li>Lesson 7: Search</li>
<li>Lesson 8: Simulated Annealing</li>
<li>Lesson 9: Constraint Satisfaction</li>
</ul>
<h2 id="Logic-Reasoning-and-Planning"><a href="#Logic-Reasoning-and-Planning" class="headerlink" title="Logic, Reasoning and Planning"></a>Logic, Reasoning and Planning</h2><ul>
<li>Lesson 10: : Logic and Reasoning</li>
<li>Lesson 11: Planning(<em>Project Due on Apr 14th</em>)<h2 id="Probabilistic-Models"><a href="#Probabilistic-Models" class="headerlink" title="Probabilistic Models"></a>Probabilistic Models</h2></li>
<li>Lesson 12: Coming soon (<em>Project Due on May 12th</em>)</li>
</ul>
<h1 id="项目概览"><a href="#项目概览" class="headerlink" title="项目概览"></a>项目概览</h1><h2 id="Term-1"><a href="#Term-1" class="headerlink" title="Term 1"></a>Term 1</h2><ul>
<li>Sudoku AI Agent</li>
<li>Game-playing AI (Isolation)</li>
<li>Pac-man Playing AI (<em>Self-assessed Project</em>)</li>
<li>Air Cargo Planning</li>
<li>Sign Language Translation</li>
</ul>
<h2 id="Term-2-choose-one"><a href="#Term-2-choose-one" class="headerlink" title="Term 2 (choose one)"></a>Term 2 (choose one)</h2><ul>
<li>Amazon Alexa: speech-to-text</li>
<li>IBM Watson: understanding sentences</li>
<li>Affectiva: classify and detect objects</li>
</ul>
<p>作为AIND二月班的成员，我<a href="https://medium.com/@xiaozhenliu/yes-i-am-also-learning-ai-nice-to-meet-you-e1441e0d7fb4" target="_blank" rel="external">在Medium上开了个记录课程的博客</a> 。相对于这里的几乎纯技术的记录，我想试试看用英文讲些故事。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.udacity.com/ai&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Artificial Intelligence Nanodegree (AIND) &lt;/a&gt;是优达学城近期新推出的纳米学位课程。与&lt;a h
    
    </summary>
    
    
      <category term="Udacity" scheme="http://tech.liuxiaozhen.com/tags/Udacity/"/>
    
      <category term="Artificial Intelligence" scheme="http://tech.liuxiaozhen.com/tags/Artificial-Intelligence/"/>
    
      <category term="Nanodegree" scheme="http://tech.liuxiaozhen.com/tags/Nanodegree/"/>
    
      <category term="AIND" scheme="http://tech.liuxiaozhen.com/tags/AIND/"/>
    
  </entry>
  
  <entry>
    <title>Small Things Matter: 更改Jupyter主题</title>
    <link href="http://tech.liuxiaozhen.com/2017/01/30/jupyter-themer/"/>
    <id>http://tech.liuxiaozhen.com/2017/01/30/jupyter-themer/</id>
    <published>2017-01-30T09:51:53.000Z</published>
    <updated>2017-01-30T09:53:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>很长时间没写有活人气息的博客了。今年想好好把这里拾掇拾掇。会慢慢更新一些以前做的笔记，同时也补充上新的内容。（Flag已立）</p>
<p>不过今天的重点是更改Jupyter主题。</p>
<p>Jupyter Notebook 作为神器已经无需赘言。然而喜欢customize一切的程序员们又怎会让它保持朴素无华的外观。Jupyter-themer就是一个可以更改其风格的软件包。安装只需一步：</p>
<p><code>$ pip install jupyter-themer</code></p>
<p>或</p>
<p><code>$ python setup.py install</code></p>
<p>Jupyter-themer可以更改代码颜色、背景风格、排版和字体。举例：</p>
<p><code>$ jupyter-themer -b dark -c erlang-dark -l wide -t serif</code></p>
<p><img src="jupyter-themer/jupyter-themer-view.png" alt=""></p>
<p>详细使用说明见：</p>
<p><a href="https://pypi.python.org/pypi/jupyter-themer" target="_blank" rel="external">Python官网上的项目主页</a></p>
<p><a href="https://github.com/transcranial/jupyter-themer" target="_blank" rel="external">GitHub上的项目主页</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很长时间没写有活人气息的博客了。今年想好好把这里拾掇拾掇。会慢慢更新一些以前做的笔记，同时也补充上新的内容。（Flag已立）&lt;/p&gt;
&lt;p&gt;不过今天的重点是更改Jupyter主题。&lt;/p&gt;
&lt;p&gt;Jupyter Notebook 作为神器已经无需赘言。然而喜欢customi
    
    </summary>
    
    
      <category term="jupyter" scheme="http://tech.liuxiaozhen.com/tags/jupyter/"/>
    
      <category term="python" scheme="http://tech.liuxiaozhen.com/tags/python/"/>
    
      <category term="theme" scheme="http://tech.liuxiaozhen.com/tags/theme/"/>
    
      <category term="small things" scheme="http://tech.liuxiaozhen.com/tags/small-things/"/>
    
  </entry>
  
  <entry>
    <title>机器学习工程师之路（1）</title>
    <link href="http://tech.liuxiaozhen.com/2016/07/06/Udacity-MLND-01/"/>
    <id>http://tech.liuxiaozhen.com/2016/07/06/Udacity-MLND-01/</id>
    <published>2016-07-06T02:48:30.000Z</published>
    <updated>2016-08-28T08:22:49.000Z</updated>
    
    <content type="html"><![CDATA[<p>项目简介和机器学习入门。包括：人工智能需解决的主要问题，人工智能的几种类型，数据科学家所需技能，机器学习的主要方法及应用。</p>
<a id="more"></a>
<p><strong>本文是纪录参与Udacity（在华名称“优达学城”）的“机器学习工程师纳米学位”项目的第一篇。</strong></p>
<p><strong>本系列全部文章见<a href="http://tech.liuxiaozhen.com/tags/MLND/">Tag: MLND</a> 。</strong></p>
<p>关于Udacity和纳米学位（简称MLND），这里不再多做介绍，可以参考前篇：<a href="http://tech.liuxiaozhen.com/2016/04/27/udacity-data-analyst-nanodegree-0/">数据分析师养成计划（1）</a>。</p>
<h1 id="Welcome-to-the-Nanodegree"><a href="#Welcome-to-the-Nanodegree" class="headerlink" title="Welcome to the Nanodegree"></a>Welcome to the Nanodegree</h1><ul>
<li>Readiness Questions: statistics, calculus, linear algebra</li>
<li>Quiz</li>
</ul>
<h2 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning"></a>What is Machine Learning</h2><ul>
<li>Methods to Processing data</li>
<li>Don’t have to explicitly tell the computer what to do to get a solution</li>
<li>Large amount of data makes Machine Learning useful</li>
</ul>
<h2 id="Program-Outline"><a href="#Program-Outline" class="headerlink" title="Program Outline"></a>Program Outline</h2><ul>
<li>How to use the ML techniques and when to use them</li>
</ul>
<h2 id="What-ML-can-do"><a href="#What-ML-can-do" class="headerlink" title="What ML can do"></a>What ML can do</h2><h3 id="Conundrums-in-AI"><a href="#Conundrums-in-AI" class="headerlink" title="Conundrums in AI"></a>Conundrums in AI</h3><ul>
<li>Intelligent agents have limited resources</li>
<li>Computation is local, but problems have global constraints.</li>
<li>Logic is deductive, but many problems are not</li>
<li>The world is dynamic, but knowledge is limited</li>
<li>Problem solving, reasoning, and learning are complex, but explanation and justification are even more complex.</li>
</ul>
<h3 id="Characteristics-of-AI-Problems"><a href="#Characteristics-of-AI-Problems" class="headerlink" title="Characteristics of AI Problems"></a>Characteristics of AI Problems</h3><ul>
<li>incrementally</li>
<li>recurring</li>
<li>multiple levels of granularity</li>
<li>computationally intractable</li>
<li>dynamic world vs. static knowledge</li>
<li>open-ended world vs. limited knowledge</li>
</ul>
<h3 id="AI-and-Uncertainty"><a href="#AI-and-Uncertainty" class="headerlink" title="AI and Uncertainty"></a>AI and Uncertainty</h3><p>AI as uncertainty Management<br>AI = what to do when you don’t know what to do </p>
<p>Reasons for uncertainty: sensor limits, adversaries, stochastic environments, laziness, ignorance</p>
<h3 id="Knowledge-based-AI"><a href="#Knowledge-based-AI" class="headerlink" title="Knowledge-based AI"></a>Knowledge-based AI</h3><p>Watson (the IBM program that plays Jeopardy)</p>
<ul>
<li>Reasoning</li>
<li>Learning</li>
<li>Memory</li>
</ul>
<p><img src="/Udacity-MLND-01/pic01.png" alt="pic01"></p>
<h3 id="The-Four-School-of-AI"><a href="#The-Four-School-of-AI" class="headerlink" title="The Four School of AI"></a>The Four School of AI</h3><p><img src="/Udacity-MLND-01/pic02.png" alt="pic02"></p>
<h3 id="Bayes-Rule"><a href="#Bayes-Rule" class="headerlink" title="Bayes Rule"></a>Bayes Rule</h3><script type="math/tex; mode=display">P(A|B) = \frac{P(B|A)*P(A)}{P(B)}</script><p>Posterior = likelihood times prior / probability of the evidence (marginal likelihood)</p>
<p><img src="/Udacity-MLND-01/pic03.png" alt="pic03"></p>
<h3 id="What-is-a-data-scientist"><a href="#What-is-a-data-scientist" class="headerlink" title="What is a data scientist?"></a>What is a data scientist?</h3><p><img src="/Udacity-MLND-01/pic04.png" alt="pic04"></p>
<h3 id="What-does-a-data-scientist-do"><a href="#What-does-a-data-scientist-do" class="headerlink" title="What does a data scientist do?"></a>What does a data scientist do?</h3><ul>
<li><p>Collect raw data and Process data -&gt; dataset</p>
</li>
<li><p>Make statistical models/analysis<br>or make machine learning predictions</p>
</li>
<li><p>Build data-driven products or publish reports, visualization, or blogs</p>
</li>
</ul>
<h3 id="Basic-Data-Scientist-Skills"><a href="#Basic-Data-Scientist-Skills" class="headerlink" title="Basic Data Scientist Skills"></a>Basic Data Scientist Skills</h3><ul>
<li>knows which questions to ask</li>
<li>can interpret the data well</li>
<li>understands structure of the data</li>
<li>data scientists often work in teams</li>
</ul>
<h3 id="Problems-solved-by-data-science"><a href="#Problems-solved-by-data-science" class="headerlink" title="Problems solved by data science"></a>Problems solved by data science</h3><ul>
<li>Netflix</li>
<li>social media</li>
<li>web apps</li>
<li>Bioinformatics</li>
<li>Urban planning</li>
<li>Astrophysics</li>
<li>Public health</li>
<li>Sports</li>
</ul>
<h3 id="Definition-of-ML"><a href="#Definition-of-ML" class="headerlink" title="Definition of ML"></a>Definition of ML</h3><p>Theoretical vs. Practical</p>
<h3 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h3><p>data are not labeled</p>
<p>assumptions<br>generalizaton<br>induction bias</p>
<p>-&gt;function approximation</p>
<h3 id="Induction-Deduction-and-Abduction"><a href="#Induction-Deduction-and-Abduction" class="headerlink" title="Induction, Deduction and Abduction"></a>Induction, Deduction and Abduction</h3><p>Fundamental forms of Inferences: Induction, Deduction and Abduction</p>
<ul>
<li><p>Deduction: given the rule and the cause, deduce the effect</p>
</li>
<li><p>Induction: given a cause and an effect, induce a rule.</p>
</li>
<li><p>Abduction: given a rule and an effect, abduce a cause</p>
</li>
</ul>
<h3 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h3><p>Description<br><img src="/Udacity-MLND-01/pic05.png" alt="pic05"></p>
<h3 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h3><p>delayed feedback</p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><ul>
<li>auto-driving cars</li>
<li>healthcare</li>
<li>internet of things</li>
<li>new drug discovery</li>
<li>trading</li>
</ul>
<h3 id="Nuts-and-Bolts"><a href="#Nuts-and-Bolts" class="headerlink" title="Nuts and Bolts"></a>Nuts and Bolts</h3><ul>
<li><p>Taxonomy</p>
<ul>
<li><p>what is being learned: prameters, structure, hidden concepts,</p>
</li>
<li><p>what from: supervised /unsupervised/ reinforcement</p>
</li>
<li><p>what for: prediction diagnostics summarization</p>
</li>
<li><p>How: passive, active, online, offline</p>
</li>
<li>outputs: classification, regression, </li>
<li>details: generative, discriminative</li>
</ul>
</li>
</ul>
<h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><p><img src="/Udacity-MLND-01/pic06.png" alt="pic06"></p>
<p>Occam’s razor: everything else equal, choose the less complex hypothesis.</p>
<p>Bayes variance methods: mathematically calculate overfitting error.</p>
<p>In practice, we will be only given training error.</p>
<p><img src="/Udacity-MLND-01/pic07.png" alt="pic07"></p>
<p><strong>Spam detection</strong><br>Bag of words: represents count of words</p>
<p>classification/regression</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;项目简介和机器学习入门。包括：人工智能需解决的主要问题，人工智能的几种类型，数据科学家所需技能，机器学习的主要方法及应用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Udacity" scheme="http://tech.liuxiaozhen.com/tags/Udacity/"/>
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
      <category term="Nanodegree" scheme="http://tech.liuxiaozhen.com/tags/Nanodegree/"/>
    
      <category term="Statistics" scheme="http://tech.liuxiaozhen.com/tags/Statistics/"/>
    
      <category term="MLND" scheme="http://tech.liuxiaozhen.com/tags/MLND/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning By Stanford University Week 6</title>
    <link href="http://tech.liuxiaozhen.com/2016/06/25/Machine-Learning-W6/"/>
    <id>http://tech.liuxiaozhen.com/2016/06/25/Machine-Learning-W6/</id>
    <published>2016-06-25T14:24:12.000Z</published>
    <updated>2016-11-11T03:51:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="6-Machine-Learning-Application-Advice-and-System-Design"><a href="#6-Machine-Learning-Application-Advice-and-System-Design" class="headerlink" title="6. Machine Learning Application Advice and System Design"></a>6. Machine Learning Application Advice and System Design</h1><p>本周课程介绍了机器学习应用中的注意事项以及系统设计。</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course “Machine Learning” taught by Andrew Ng on Coursera.org, 2016 May session.</strong></p>
<h2 id="6-1-Evaluating-a-Learning-Algorithm"><a href="#6-1-Evaluating-a-Learning-Algorithm" class="headerlink" title="6.1 Evaluating a Learning Algorithm"></a>6.1 Evaluating a Learning Algorithm</h2><p>Goal: Learn How to choose one of the most promising avenues to spend your time pursuing. </p>
<h3 id="6-1-1-Deciding-what-to-try-next"><a href="#6-1-1-Deciding-what-to-try-next" class="headerlink" title="6.1.1 Deciding what to try next"></a>6.1.1 Deciding what to try next</h3><p><strong>Debugging a learning algorithm</strong></p>
<p>Suppose you have implemented regularized linear regression to predict housing prices. </p>
<p>However, when you test your hypothesis on a new set of hourses, you find that it makes unacceptably large errors in its predictions. What should you try next?</p>
<ul>
<li>Get more training examples</li>
<li>Try smaller sets of features </li>
<li>Try getting additional features</li>
<li>Try adding polynomial features $(x_1^2, x_2^2, x_1x_2, \text{etc})$</li>
<li>Try decreasing $\lambda$</li>
<li>Try increasing $\lambda$  </li>
</ul>
<p>WRONG: Randomly pick one of these options</p>
<p>CORRECT: Machine Learning Diagnostic</p>
<p>A test that you can run to gain insight what is/isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance. </p>
<h3 id="6-1-2-Evaluating-a-Hypothesis"><a href="#6-1-2-Evaluating-a-Hypothesis" class="headerlink" title="6.1.2 Evaluating a Hypothesis"></a>6.1.2 Evaluating a Hypothesis</h3><p>Example: Using housing sizes to predict prices.</p>
<p>Method: (randomly) choose 70% of the dataset (samples) as the training set, the other 30% as the test set. </p>
<p>Training set: $m$ number of samples; $x^{(1)},y^{(1)}…x^{(m)},y^{(m)}$</p>
<p>Test set: $m<em>test$ number of samples<br>$x</em>{test}^{(1)},y<em>{test}^{(1)}…x</em>{test}^{(m)},y_{text}^{(m)}$</p>
<p><strong>Training/testing procedure for linear regression</strong></p>
<ul>
<li>Learn parameter $\theta$ from training data (minimizing training error $J(\theta)$)</li>
<li>Compute test set error: </li>
</ul>
<script type="math/tex; mode=display">J_{test}(\theta)=\frac1{2m_{test}}\sum_{i=1}^{m_{test}}\left(h_\theta(x_{test}^{(i)})-y_{test}^{(i)}\right)^2</script><script type="math/tex; mode=display">J_{test}(\theta)=\frac1{2m_{test}}\sum_{i=1}^{m_{test}}y_{test}^{(i)}\log h_\theta(x_{test}^{(i)})+(1-y_{test}^{(i)})\log h_\theta(x_{test}^{(i)})</script><ul>
<li>Misclassification error (0/1 misclassification error);</li>
</ul>
<script type="math/tex; mode=display">err(h_\theta(x),y)=\begin{cases}1\quad\text{if }h_\theta(x)\geq0.5, y=0\text{ or if } h_\theta(x)\leq0.5, y=1\\0\quad\text{otherwise}\end{cases}</script><script type="math/tex; mode=display">\text{Test error} = \frac1{m_{test}}\sum_{i=1}^{m_{test}}err\left(h_\theta(x_{test}^{(i)}),y_{test}^{(i)}\right)\</script><h3 id="6-1-3-Model-Selection-and-Train-Validation-Test-Sets"><a href="#6-1-3-Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="6.1.3 Model Selection and Train/Validation/Test Sets"></a>6.1.3 Model Selection and Train/Validation/Test Sets</h3><p><strong>Overfitting example</strong></p>
<p>Once parameters $\theta_0,\theta_1,…,\theta_4$ were fit to some set of data (traning set), the error of the parameters as measured on that data (the traing error $J(\theta)$) is likely to be lower than the actual generalization error.</p>
<p><strong>Model selection</strong></p>
<ol>
<li>$h_\theta(x) = \theta_0+\theta_1x$</li>
<li>$h_\theta(x) = \theta_0+\theta_1x+\theta_2x^2$</li>
<li>$h_\theta(x) = \theta_0+\theta_1x+…+\theta_3x^2$<br>…</li>
<li>$h_\theta(x) = \theta_0+\theta_1x+…+\theta_10x^10$</li>
</ol>
<p>Assume a new parameter $d$ = degree of polynomial. </p>
<p>Test dataset cost functions: $J_{test}(\theta_j)$ ($j$ is from 1 to $d$)</p>
<p>Problem: degree of polynomial is chosen based on the test dataset, therefore cannot guarantee the generalization ability to further data. </p>
<p><strong>Solution</strong>: divide the original dataset to a training set, a cross validation set (CV), and a test set. </p>
<p><strong>Train/validation/test error</strong></p>
<p>Use the CV set to select the model. Then estimate generalization error $J_{test}(\theta)$</p>
<h2 id="6-2-Bias-vs-Variance"><a href="#6-2-Bias-vs-Variance" class="headerlink" title="6.2 Bias vs. Variance"></a>6.2 Bias vs. Variance</h2><h3 id="6-2-1-Diagnosing-Bias-vs-Variance"><a href="#6-2-1-Diagnosing-Bias-vs-Variance" class="headerlink" title="6.2.1 Diagnosing Bias vs. Variance"></a>6.2.1 Diagnosing Bias vs. Variance</h3><p>High bias - Underfit<br>High variance - Overfit</p>
<p>Training error: </p>
<script type="math/tex; mode=display">J_{train}(\theta)=\frac1{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2</script><p>Cross validation error: </p>
<script type="math/tex; mode=display">J_{cv}(\theta)=\frac1{2m_{cv}}\sum_{i=1}^{m_{cv}}\left(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)}\right)^2</script><p><img src="Machine-Learning-W6/Coursera-ml-w6-01.png" alt=""></p>
<h3 id="6-2-2-Regularization-and-Bias-Variance"><a href="#6-2-2-Regularization-and-Bias-Variance" class="headerlink" title="6.2.2 Regularization and Bias/Variance"></a>6.2.2 Regularization and Bias/Variance</h3><p><strong>Linear regression with regularization</strong></p>
<p>Large $\lambda$: high bias, underfit</p>
<p>Small $\lambda$: high variance, overfit</p>
<p><strong>Choosing the regularization parameter $\lambda$</strong></p>
<ul>
<li><p>have some range of $\lambda$ values: e.g. [0,0.01,0.02,0.04,0.08…10]</p>
</li>
<li><p>calculate $\min J(\theta)$ for each $\lambda$ -&gt; $J_{cv}(\theta^{(?)})$</p>
</li>
<li><p>pick the lowest $J<em>{cv}(\theta^{(?)})$ $\theta^{(5)}$. Test error: $J</em>{test}(\theta^{(5)})$</p>
</li>
<li><p>Plot $J<em>{train}(\theta)$ and $J</em>{cv}(\theta)$ v.s. $\lambda$</p>
</li>
</ul>
<p><img src="Machine-Learning-W6/Coursera-ml-w6-02.png" alt=""></p>
<h3 id="6-2-3-Learning-Curves"><a href="#6-2-3-Learning-Curves" class="headerlink" title="6.2.3 Learning Curves"></a>6.2.3 Learning Curves</h3><p>增加样本数是否能减少误差？</p>
<p>Learning curves: plot error v.s. m (training set size)</p>
<p><img src="Machine-Learning-W6/Coursera-ml-w6-03.png" alt=""></p>
<p><img src="Machine-Learning-W6/Coursera-ml-w6-04.png" alt=""></p>
<p>High bias:</p>
<p>If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.</p>
<p><img src="Machine-Learning-W6/Coursera-ml-w6-05.png" alt=""></p>
<p>High variance:</p>
<p>If a learning algorithm is suffering from high variance, getting more traning data is likely to help.</p>
<p><img src="Machine-Learning-W6/Coursera-ml-w6-06.png" alt=""></p>
<h3 id="6-2-4-Deciding-What-to-Do-Next-Revisited"><a href="#6-2-4-Deciding-What-to-Do-Next-Revisited" class="headerlink" title="6.2.4 Deciding What to Do Next Revisited"></a>6.2.4 Deciding What to Do Next Revisited</h3><p><strong>Debugging a learning algorithm</strong></p>
<p>Suppose you have implemented regularized linear regression to predict housing prices. </p>
<p>However, when you test your hypothesis on a new set of hourses, you find that it makes unacceptably large errors in its predictions. What should you try next?</p>
<ul>
<li>Get more training examples -&gt; <em>fixes high variance</em></li>
<li>Try smaller sets of features -&gt; <em>fixes high variance</em></li>
<li>Try getting additional features -&gt; <em>fixes high bias</em></li>
<li>Try adding polynomial features $(x_1^2, x_2^2, x_1x_2, \text{etc})$ -&gt; <em>fixes high bias</em></li>
<li>Try decreasing $\lambda$ -&gt; <em>fixes high bias</em></li>
<li>Try increasing $\lambda$ -&gt; <em>fixes high variance</em></li>
</ul>
<p><strong>Neural networks and overfitting</strong></p>
<p><img src="Machine-Learning-W6/Coursera-ml-w6-06.png" alt=""></p>
<p>Using a single hidden layer is a reasonable default. But if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets. </p>
<h2 id="6-3-Building-a-Spam-Classifier"><a href="#6-3-Building-a-Spam-Classifier" class="headerlink" title="6.3 Building a Spam Classifier"></a>6.3 Building a Spam Classifier</h2><h3 id="6-3-1-Prioritizing-What-to-Work-on"><a href="#6-3-1-Prioritizing-What-to-Work-on" class="headerlink" title="6.3.1 Prioritizing What to Work on"></a>6.3.1 Prioritizing What to Work on</h3><h3 id="6-3-2-Error-Analysis"><a href="#6-3-2-Error-Analysis" class="headerlink" title="6.3.2 Error Analysis"></a>6.3.2 Error Analysis</h3><h2 id="6-4-Handling-Skewed-Data"><a href="#6-4-Handling-Skewed-Data" class="headerlink" title="6.4 Handling Skewed Data"></a>6.4 Handling Skewed Data</h2><p>Cancer classification example<br>1% error on test set (99% correct)</p>
<h3 id="Error-Metrics-for-Skewed-Classes"><a href="#Error-Metrics-for-Skewed-Classes" class="headerlink" title="Error Metrics for Skewed Classes"></a>Error Metrics for Skewed Classes</h3><h3 id="Trading-Off-Precision-and-Recall"><a href="#Trading-Off-Precision-and-Recall" class="headerlink" title="Trading Off Precision and Recall"></a>Trading Off Precision and Recall</h3><p>Logistic regression:<br>Predict 1 if<br>Predict 0 if<br>Suppose we want to predict y = 1 only if very confident</p>
<p>-&gt; Higher precision, lower recall.</p>
<p>Suppose we want to avoid missing too many cases of cancer (avoid false negatives).</p>
<p>-&gt; higher recall, lower precision</p>
<p><strong>$F_1$ Score (F score)</strong></p>
<p>How to compare precision/recall numbers?</p>
<p>The average of P and R is not a good indicator</p>
<p>$F_1$ score: $2\frac{PR}{P+R}$</p>
<h2 id="6-5-Using-Large-Data-Sets"><a href="#6-5-Using-Large-Data-Sets" class="headerlink" title="6.5 Using Large Data Sets"></a>6.5 Using Large Data Sets</h2><h3 id="Designing-a-high-accuracy-learning-system"><a href="#Designing-a-high-accuracy-learning-system" class="headerlink" title="Designing a high accuracy learning system"></a>Designing a high accuracy learning system</h3><p>e.g. Classify between confusable words. {to,two,too},{then,than}</p>
<p>Algorithms:</p>
<ul>
<li>Perceptron (Logistic regression)</li>
<li>Winnow</li>
<li>Memory-based</li>
<li>Naive Bayes</li>
</ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;6-Machine-Learning-Application-Advice-and-System-Design&quot;&gt;&lt;a href=&quot;#6-Machine-Learning-Application-Advice-and-System-Design&quot; class=&quot;headerlink&quot; title=&quot;6. Machine Learning Application Advice and System Design&quot;&gt;&lt;/a&gt;6. Machine Learning Application Advice and System Design&lt;/h1&gt;&lt;p&gt;本周课程介绍了机器学习应用中的注意事项以及系统设计。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Learning Curves" scheme="http://tech.liuxiaozhen.com/tags/Learning-Curves/"/>
    
      <category term="Model Selection" scheme="http://tech.liuxiaozhen.com/tags/Model-Selection/"/>
    
      <category term="Overfitting" scheme="http://tech.liuxiaozhen.com/tags/Overfitting/"/>
    
      <category term="Precision" scheme="http://tech.liuxiaozhen.com/tags/Precision/"/>
    
      <category term="Recall" scheme="http://tech.liuxiaozhen.com/tags/Recall/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning By Stanford University Week 5</title>
    <link href="http://tech.liuxiaozhen.com/2016/06/12/Machine-Learning-W5/"/>
    <id>http://tech.liuxiaozhen.com/2016/06/12/Machine-Learning-W5/</id>
    <published>2016-06-12T14:49:53.000Z</published>
    <updated>2017-02-25T07:25:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="5-Neural-Networks-Learning"><a href="#5-Neural-Networks-Learning" class="headerlink" title="5. Neural Networks: Learning"></a>5. Neural Networks: Learning</h1><p>本课回顾了神经网络的模型描述，并介绍了BP神经网络的训练过程、代价函数表达式、梯度下降算法、以及随机初始化。</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course “Machine Learning” taught by Andrew Ng on Coursera.org, 2016 May session.</strong></p>
<h2 id="5-1-Neural-Networks-Classification"><a href="#5-1-Neural-Networks-Classification" class="headerlink" title="5.1 Neural Networks (Classification)"></a>5.1 Neural Networks (Classification)</h2><p>Training set ${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$</p>
<p><img src="Machine-Learning-W5/Coursera-ml-w5-01.png" alt=""></p>
<p>L = total no. of layers in network. (Here L = 4)</p>
<p>$s_l$ = no. of units (not counting bias unit) in layer $l$</p>
<p>(Here $s_1 = 3$, $s_2 = 5$,$s_4 = s_l =4$)</p>
<p>For binary classification, there is only 1 output unit; therefore $s_l = 1$. For multi-class classification (K classes), $s_l = K$. ($K\geq3$) </p>
<h3 id="5-1-1-Cost-functions-代价函数"><a href="#5-1-1-Cost-functions-代价函数" class="headerlink" title="5.1.1 Cost functions 代价函数"></a>5.1.1 Cost functions 代价函数</h3><p><strong>a) Logistic regression</strong></p>
<script type="math/tex; mode=display">J(\theta)=-\frac1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log\left(1-h_\theta(x^{(i)})\right)\right] + \frac\lambda{2m}\sum_{j=1}^n\theta_j^2</script><p><strong>b) Neural network</strong></p>
<script type="math/tex; mode=display">h_\Theta(x)\in \mathbb{R}^K\qquad(h_\Theta(x))_i = i^{th}  \space\mathrm{output}</script><script type="math/tex; mode=display">
J(\Theta)=-\frac1m\left[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\right]+\frac\lambda{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2</script><h3 id="5-1-2-Backpropagation-Algorithm"><a href="#5-1-2-Backpropagation-Algorithm" class="headerlink" title="5.1.2 Backpropagation Algorithm"></a>5.1.2 Backpropagation Algorithm</h3><p>为了最小化误差，需要找到使代价函数 $j(\theta)$ 最小的 $\theta$. 因此，需要在代码中计算 $j(\theta)$ 及其微分式。</p>
<p><strong>a) Gradient Computation</strong></p>
<p>考虑最简单情况，仅有一个训练样本：<br>Given one training example ($x,y$):</p>
<p><img src="Machine-Learning-W5/Coursera-ml-w5-02.png" alt=""></p>
<p><strong>b) Forward propagation计算步骤</strong></p>
<ul>
<li>$a^{(1)} = x$</li>
<li>$z^{(2)} = \Theta^{(1)}a^{(1)}$</li>
<li>$a^{(2)}=g(z^{(2)})\quad(\mathrm{add}\space a_0^{(2)})$</li>
<li>$z^{(3)} = \Theta^{(2)}a^{(2)}$</li>
<li>$a^{(3)}=g(z^{(3)})\quad(\mathrm{add}\space a_0^{(3)})$</li>
<li>$z^{(4)} = \Theta^{(3)}a^{(3)}$</li>
<li>$a^{(4)}=h_\Theta(x)=g(z^{(4)})$</li>
</ul>
<p><strong>c) Backpropagation algorithm</strong></p>
<p>Intuition: $\delta_j^{(l)} = $ “error” of node $j$ in layer $l$.</p>
<p>设 $\delta_j^{(l)}$ 为第 $l$ 层第 $j$ 个神经元的误差值。</p>
<p>For each output unit (layer L = 4)</p>
<ul>
<li>$\delta<em>j^{(4)}=a_j^{(4)}-y_j=(h</em>\Theta(x))_j-y_j$</li>
</ul>
<p>Vector form:</p>
<ul>
<li><p>$\delta^{(4)}=a^{(4)}-y$</p>
</li>
<li><p>$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}.<em>g’(z^{(3)}),\quad g’(z^{(3)})=a^{(3)}.</em>(1-a^{(3)})$</p>
</li>
<li><p>$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}.<em>g’(z^{(2)}),\quad g’(z^{(2)})=a^{(2)}.</em>(1-a^{(2)})$</p>
</li>
<li><p>(There is No $\delta^{(1)}$!)</p>
</li>
</ul>
<p><strong>d) Backpropagation implementation</strong></p>
<ul>
<li><p>Set $\Delta_{ij}^{(l)}=0$ (for all $l,i,j$).</p>
</li>
<li><p>For $i = 1$ to $m$</p>
<ul>
<li>设置初始激活值 Set $a^{(1)}=x^{(i)}$</li>
<li>正向计算各层激活值 Perform forward propagation to compute $a^{(l)}$ for $l=2,3,…,L$</li>
<li>用输出结果，计算最终层误差 Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$</li>
<li>反向计算其他层的误差 Compute $\delta^{(L-1)}, \delta^{(L-2)},\delta^{(2)}$</li>
<li><p>$\Delta<em>{ij}^{(l)}:=\Delta</em>{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$   </p>
<p> Here, vector form: </p>
<p> $\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$  </p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\frac\partial{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}</script><ul>
<li>代价函数的微分式<br>$D<em>{ij}^{(l)}:=\frac1m\Delta</em>{ij}^{(l)}+\lambda\Delta<em>{ij}^{(l)}$ if $j\neq0$<br>$D</em>{ij}^{(l)}:= \frac1m\Delta_{ij}^{(l)}\qquad$ if $j=0$</li>
</ul>
<h3 id="5-1-3-Backpropagation-Intuition"><a href="#5-1-3-Backpropagation-Intuition" class="headerlink" title="5.1.3 Backpropagation Intuition"></a>5.1.3 Backpropagation Intuition</h3><p><strong>a) Forward Propagation</strong></p>
<p><img src="Machine-Learning-W5/Coursera-ml-w5-03.png" alt=""></p>
<p><strong>b) What is backpropagation doing</strong></p>
<p>Focosing on a single example $x^{(i)}, y^{(i)}$, the case of 1 output unit, and ignoring regularization ($\lambda=0$),</p>
<script type="math/tex; mode=display">
cost(i)=y^{(i)}\log h_\Theta(x^{(i)})+(1-y^{(i)})\log h_\Theta(x^{(i)})</script><p>(Think of $cost(i)\approx(h_\Theta(x^{(i)})-y^{(i)})^2$)</p>
<p>i.e. how well is the network doing on example i?</p>
<p><img src="Machine-Learning-W5/Coursera-ml-w5-04.png" alt=""></p>
<h2 id="5-2-Backpropagation-Implementation-Notes"><a href="#5-2-Backpropagation-Implementation-Notes" class="headerlink" title="5.2 Backpropagation Implementation Notes"></a>5.2 Backpropagation Implementation Notes</h2><h3 id="5-2-1-Unrolling-Parameters"><a href="#5-2-1-Unrolling-Parameters" class="headerlink" title="5.2.1 Unrolling Parameters"></a>5.2.1 Unrolling Parameters</h3><p><strong>a) Advanced Optimization</strong></p>
<p><code>function [jVal, gradient] = costFunction(theta)</code><br>…<br><code>optTheta = fminunc(@costFunction, initialTheta, options)</code></p>
<p>Neutal Network(L=4):<br>$\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$ - matrices(<code>Theta1, Theta2, Theta3</code>)<br>$D^{(1)},D^{(2)},D^{(3)}$ - matrices (<code>D1, D2, D3</code>)</p>
<p>“Unroll” into vectors</p>
<p><strong>b) Example</strong></p>
<p>$s_1 = 10,s_2 = 10, s_3=1$</p>
<p>$\Theta^{(1)}\in \mathbb{R}^{10\times11}, \Theta^{(2)}\in \mathbb{R}^{10\times11},\Theta^{(3)}\in \mathbb{R}^{1\times11}$</p>
<p>$D^{(1)}\in \mathbb{R}^{10\times11}, D^{(2)}\in \mathbb{R}^{10\times11},D^{(3)}\in \mathbb{R}^{1\times11}$</p>
<p><code>thetaVec = [ Theta1(:); THeta2(:); Theta3(:)];</code><br><code>DVec = [D1(:); D2(:); D3(:)];</code></p>
<p><code>Theta1 = reshape(thetaVec(1:110),10,11);</code><br><code>Theta2 = reshape(thetaVec(111:220),10,11);</code><br><code>Theta3 = reshape(thetaVec(221:231),1,11);</code></p>
<p><strong>c) Learning Algorithm</strong></p>
<ul>
<li><p>Have initial parameters $\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$.</p>
</li>
<li><p>Unroll to get <code>initialTheta</code> to pass to <code>fminunc(@costFunction, initialTheta, options)</code></p>
</li>
<li><p><code>function [jval, gradientVec] = costFunction(thetaVec)</code></p>
</li>
<li><p>From <code>thetaVec</code>, get $\Theta^{(1)},\Theta^{(2)},\Theta^{(3)}$.</p>
</li>
<li><p>Use forward prop/back prop to compute $D^{(1)},D^{(2)},D^{(3)}$ and $J(\Theta)$.</p>
</li>
<li>Unroll $D^{(1)},D^{(2)},D^{(3)}$ to get <code>gradientVec</code>.</li>
</ul>
<h3 id="5-2-2-Gradient-Checking"><a href="#5-2-2-Gradient-Checking" class="headerlink" title="5.2.2 Gradient Checking"></a>5.2.2 Gradient Checking</h3><p>In order to verify that the neural network algorithm is working properly, one needs to do gradient checking. </p>
<p><strong>a) Numerial estimation of gradients</strong></p>
<p><img src="Machine-Learning-W5/Coursera-ml-w5-05.png" alt=""></p>
<p>Implement:<br><code>gradApprox = (J(theta + EPASILON) - J(theta - EPSILON))/(2*EPSILON)</code></p>
<p><strong>b) Parameter vector $\theta$</strong></p>
<p><img src="Machine-Learning-W5/Coursera-ml-w5-06.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">for i = 1:n,</div><div class="line">	thetaPlus = theta;</div><div class="line">	thetaPlus(i) = thetaPlus(i) + EPSILON;</div><div class="line">	thetaMinus = theta;</div><div class="line">	thetaMinus(i) = thetaMinus(i) - EPSILON;</div><div class="line">	gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*EPSILON);</div><div class="line">end;</div></pre></td></tr></table></figure>
<p>Check that <code>gradApprox</code> $\approx$ <code>DVec</code> </p>
<p><strong>c) Implementation Note:</strong></p>
<ul>
<li>Implement backprop to compute <code>DVec</code> (unrolled $D^{(1)},D^{(2)},D^{(3)}$).</li>
<li>Implement numerial gradient check to compute <code>gradApprox</code>.</li>
<li>Make sure they give similar values.</li>
<li>Turn off gradient checking. Using backprop code for learning.</li>
</ul>
<p><strong>Important:</strong></p>
<ul>
<li>Be sure to disable your gradient cheking code before training your classifier. If you run numeiral gradient computation on every iteration of gradient descent (or in the inner loop of <code>costFunction(...)</code>) your code will be very slow.</li>
</ul>
<h3 id="5-2-3-Random-Initialization"><a href="#5-2-3-Random-Initialization" class="headerlink" title="5.2.3 Random Initialization"></a>5.2.3 Random Initialization</h3><p>Need initial value for $\Theta$</p>
<ul>
<li>Zero initialization. <ul>
<li>$\Theta_{ij}^{(l)} = 0$ for all $i,j,l$</li>
<li>will cause the problem of symmetric ways: </li>
</ul>
</li>
<li>Random initialization: Symmetry breaking. <ul>
<li>initialize each $\Theta_{ij}^{(l)}$ to a random value in $[-\epsilon,\epsilon]$</li>
<li>E.g.<br><code>Theta1 = rand(10,11) * (2*INIT_EPSILON) - INIT_EPSILON;</code><br><code>Theta2 = rand(1,11) * (2*INIT_EPSILON) - INIT_EPSILON;</code></li>
</ul>
</li>
</ul>
<h2 id="5-3-Putting-It-Together"><a href="#5-3-Putting-It-Together" class="headerlink" title="5.3 Putting It Together"></a>5.3 Putting It Together</h2><h3 id="5-3-1-Training-a-neural-network"><a href="#5-3-1-Training-a-neural-network" class="headerlink" title="5.3.1 Training a neural network"></a>5.3.1 Training a neural network</h3><ul>
<li>pick a network architecture (connectivity pattern between neurons)</li>
<li>No. of input units: Dimension of features </li>
<li>No. output units: Number of classes</li>
<li>Reasonable default: 1 hidden layer, or if &gt;1 hidden layer, have same no. of hidden uints in every layer (usually the more the better, but will be more computational expensive). </li>
</ul>
<h3 id="5-3-2-Steps-of-training-a-neural-network"><a href="#5-3-2-Steps-of-training-a-neural-network" class="headerlink" title="5.3.2 Steps of training a neural network"></a>5.3.2 Steps of training a neural network</h3><ol>
<li>Randomly initialize weights</li>
<li>Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$</li>
<li>Implement code to compute cost function $J(\Theta)$</li>
<li>Implement backprop to compute partial derivatives $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$</li>
<li>Use gradient checking to compare $\frac{\partial}{\partial\Theta_{jk}^{(l)}}J(\Theta)$ computed using backpropagation v.s. using numerical estimate of gradient of $J(\Theta)$. Then disable gradient checking code.</li>
<li>Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\theta)$ as a function of parameters $\Theta$</li>
</ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;5-Neural-Networks-Learning&quot;&gt;&lt;a href=&quot;#5-Neural-Networks-Learning&quot; class=&quot;headerlink&quot; title=&quot;5. Neural Networks: Learning&quot;&gt;&lt;/a&gt;5. Neural Networks: Learning&lt;/h1&gt;&lt;p&gt;本课回顾了神经网络的模型描述，并介绍了BP神经网络的训练过程、代价函数表达式、梯度下降算法、以及随机初始化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Neural Networks" scheme="http://tech.liuxiaozhen.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning By Stanford University Week 3</title>
    <link href="http://tech.liuxiaozhen.com/2016/05/14/Machine-Learning-W3/"/>
    <id>http://tech.liuxiaozhen.com/2016/05/14/Machine-Learning-W3/</id>
    <published>2016-05-14T15:22:21.000Z</published>
    <updated>2016-08-21T10:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="3-Logistic-Regression-and-Regularization"><a href="#3-Logistic-Regression-and-Regularization" class="headerlink" title="3. Logistic Regression and Regularization"></a>3. Logistic Regression and Regularization</h1><p>本周课程介绍了逻辑回归分析（分类问题求解）及正则化方法。</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course “Machine Learning” taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="3-1-Logistic-Regression"><a href="#3-1-Logistic-Regression" class="headerlink" title="3.1 Logistic Regression"></a>3.1 Logistic Regression</h2><h3 id="3-1-1-Classification-Problems-and-Logistic-Regression-Model"><a href="#3-1-1-Classification-Problems-and-Logistic-Regression-Model" class="headerlink" title="3.1.1 Classification Problems and Logistic Regression Model"></a>3.1.1 Classification Problems and Logistic Regression Model</h3><p><strong>a) Classification Problems</strong></p>
<ul>
<li>Email： spam/not spam</li>
<li>Online transactions: froudulent or not</li>
<li>Tumor: malignant/benign</li>
</ul>
<p>$y\in{0,1}$</p>
<ul>
<li>0: “Negative Class”</li>
<li>1: “Positive Class”</li>
</ul>
<p>Use threshold classifier output $h_\theta(x)$ at 0.5</p>
<ul>
<li>If $h_\theta(x) \geq 0.5$, predict “y=1”</li>
<li>If $h_\theta(x) &lt; 0.5$, predict “y=0” </li>
</ul>
<p><strong>b) Logistic Regression Model</strong><br>Want $0\leq h_\theta(x) \leq 1$</p>
<p>$h_\theta(x) = g(\theta^Tx)$<br>The sigmoid function: $g(z) = \frac 1{1+e^{-z}}$</p>
<script type="math/tex; mode=display">h_\theta(x) =\frac 1{1+e^{-\theta^Tx}}</script><p>whenever $z \geq 0$, or $\theta^Tx \geq 0$,y =1</p>
<h3 id="3-1-2-Decision-Boundary"><a href="#3-1-2-Decision-Boundary" class="headerlink" title="3.1.2 Decision Boundary"></a>3.1.2 Decision Boundary</h3><p><strong>a) Decision Boundary</strong></p>
<p>$h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2)$</p>
<p><strong>b) Non-linear decision boundaries</strong></p>
<p>$h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2)$</p>
<p>Predict “y=1” if $-1+x_1^2+x_2^2 \geq 0$</p>
<h2 id="3-1-3-Cost-Function"><a href="#3-1-3-Cost-Function" class="headerlink" title="3.1.3 Cost Function"></a>3.1.3 Cost Function</h2><p>Logistic regression:</p>
<script type="math/tex; mode=display">\text{Cost}(h_\theta(x^{(i)}),y^{(i)}) = \frac12\left(h_\theta(x^{(i)})-y^{(i)}\right)^2</script><p>“non-convex” v.s. “convex”</p>
<script type="math/tex; mode=display">\text{Cost}(h_\theta(x),y)=\begin{cases}\quad-\log(h_\theta(x)) 
\quad\text{if } y = 1\\
-\log(1-h_\theta(x))\quad\text{if }y =0
\end{cases}</script><p>Cost = 0 if y=1, $h<em>\theta(x)=1$<br>But as $h</em>\theta(x)\rightarrow 0$, Cost $\rightarrow \infty$</p>
<p>More compact way:</p>
<p>$\text{Cost}(h<em>\theta(x),y)=-y\log\left(h</em>\theta(x)\right)-(1-y)\log\left(1-h_\theta(x)\right)<br>$</p>
<script type="math/tex; mode=display">J(\theta)= - \frac 1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]</script><p>To fit parameters $\theta$:<br>$\min_\theta J (\theta)$</p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\sum_{i=1}^m \left( h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(j)}</script><p>}</p>
<p>Algorithm looks identical to linear regression!</p>
<h3 id="3-1-4-Optimization-algorithm"><a href="#3-1-4-Optimization-algorithm" class="headerlink" title="3.1.4 Optimization algorithm"></a>3.1.4 Optimization algorithm</h3><ul>
<li>Gradient descent</li>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>Advantages:</p>
<ul>
<li><p>No need to manually pick $\alpha$</p>
</li>
<li><p>Often faster than gradient descent</p>
</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>More complex</li>
</ul>
<h3 id="3-1-5-Multiclass-Classification"><a href="#3-1-5-Multiclass-Classification" class="headerlink" title="3.1.5 Multiclass Classification"></a>3.1.5 Multiclass Classification</h3><p><strong>a) Problem examples</strong></p>
<ul>
<li>Email foldering/tagging: work, friends, family, hobby</li>
<li>Medical diagrams: not ill, cold, flu</li>
<li>Weather: sunny, cloudy, rain, snow</li>
</ul>
<p><strong>b) One-vs-all (one-vs-rest)</strong><br><img src="Machine-Learning-W3/Coursera-ml-w3-01.png" alt=""></p>
<p>将需要分为N类的数据拆分成 N个0/1（是／否）问题。</p>
<p>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$.</p>
<p>On a new input $x$, to make a prediction, pick the class $i$ that maximizes $\max h_\theta^{(i)}(x)$.</p>
<h2 id="3-2-Regularization-正则化"><a href="#3-2-Regularization-正则化" class="headerlink" title="3.2 Regularization 正则化"></a>3.2 Regularization 正则化</h2><h3 id="3-2-1-Overfitting-Problems-过拟合问题"><a href="#3-2-1-Overfitting-Problems-过拟合问题" class="headerlink" title="3.2.1 Overfitting Problems 过拟合问题"></a>3.2.1 Overfitting Problems 过拟合问题</h3><p><strong>a) Example: Linear Regression (housing prices)</strong></p>
<p><img src="Machine-Learning-W3/Coursera-ml-w3-02.png" alt=""></p>
<p>对于房屋价格的三种拟合曲线与实际数据的符合情况：</p>
<ul>
<li>一元线性表达式：underfit, high bias</li>
<li>一元二次表达式：just right</li>
<li>一元四次表达式：overfit, high variance</li>
</ul>
<p><strong>Overfitting:</strong> If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples (predict prices on new examples).</p>
<p><strong>b) Example: Logistic Regression</strong><br><img src="Machine-Learning-W3/Coursera-ml-w3-03.png" alt=""></p>
<p><strong>Addressing overfitting:</strong></p>
<ul>
<li>Reduce number of features:<ul>
<li>Manually select which features to keep</li>
<li>Model selection algorithm (later in course)</li>
</ul>
</li>
<li>Regularization:<ul>
<li>Keep all the features, but reduce magnitude/values of parameters $\theta_j$.</li>
<li>Works well when we have a lot of features, each of which contributes a bit to predicting $y$.</li>
</ul>
</li>
</ul>
<h3 id="3-2-2-Cost-Function"><a href="#3-2-2-Cost-Function" class="headerlink" title="3.2.2 Cost Function"></a>3.2.2 Cost Function</h3><p><strong>a) Intuition</strong></p>
<p><img src="Machine-Learning-W3/Coursera-ml-w3-04.png" alt=""></p>
<p>Suppose we penalize and make $\theta_3,\theta_4$ really small.</p>
<script type="math/tex; mode=display">\min_\theta\frac1{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2 +1000\theta_3^2+1000\theta_4^2</script><p>$\theta_3,\theta_4$ will be very small.</p>
<p><strong>b) Regularization</strong></p>
<p>Small values for parameters $\theta_0, \theta_1, …, \theta_n$</p>
<ul>
<li>“Simpler” hypothesis</li>
<li>Less prone to overfitting</li>
</ul>
<p><strong>Cost Function</strong></p>
<script type="math/tex; mode=display">
J(\theta) = \frac1{2m}\left[\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+\lambda\sum_{j=1}^n\theta_j^2\right]</script><p>$\theta_0$ is excluded as a convention.</p>
<p>If $\lambda$ is too large, we might have underfitting problems. </p>
<h3 id="3-2-3-Regularized-Linear-Regression"><a href="#3-2-3-Regularized-Linear-Regression" class="headerlink" title="3.2.3 Regularized Linear Regression"></a>3.2.3 Regularized Linear Regression</h3><p><strong>a) Gradient descent</strong></p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\left[\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}+\frac{\lambda}m\theta_j\right]</script><p>}</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j(1-\alpha\frac{\lambda}m)-\alpha\frac1m\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}</script><p>$1-\alpha\frac{\lambda}m&lt;1$. Therefore, use $\theta_j^2$ as an approximation</p>
<p><strong>b) Normal equation</strong></p>
<script type="math/tex; mode=display">
X = \begin{bmatrix}
(x^{(1)})^T\\
...\\
(x^{(m)})^T\\
\end{bmatrix}</script><script type="math/tex; mode=display">
y = \begin{bmatrix}
(y^{(1)})^T\\
...\\
(y^{(m)})^T\\
\end{bmatrix}</script><p>To minimize $J(\theta)$</p>
<script type="math/tex; mode=display">\theta = \left(X^TX+\lambda \begin{bmatrix}
0&0&0&...&0\\
0&1&0&...&0\\
0&0&1&...&0\\
...&...&...&1&...\\
0&0&0&...&1\\
\end{bmatrix}\right)^{-1}X^Ty</script><p><strong>c) Non-invertibility (optional/advanced)</strong></p>
<p>Suppose $m\leq n$,<br>(where m: number of examples; n: number of features)</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>$X^TX$ is non-invertible/singular. In Octave/MATLAB, use <code>pinv</code> instead of <code>inv</code>.</p>
<p>If $\lambda \gt 0,$</p>
<script type="math/tex; mode=display">
\theta = \left(X^TX+\lambda \begin{bmatrix}
0&0&0&...&0\\
0&1&0&...&0\\
0&0&1&...&0\\
...&...&...&1&...\\
0&0&0&...&1\\
\end{bmatrix}\right)^{-1}X^Ty</script><h3 id="3-2-4-Regularized-Logistic-Regression"><a href="#3-2-4-Regularized-Logistic-Regression" class="headerlink" title="3.2.4 Regularized Logistic Regression"></a>3.2.4 Regularized Logistic Regression</h3><p><strong>a) Cost function:</strong></p>
<script type="math/tex; mode=display">J(\theta)= - \frac 1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p><strong>b) Gradient descent</strong></p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\left[\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}+\frac{\lambda}m\theta_j\right]</script><p>}</p>
<p>where $h_\theta(x)=\frac1{1+e^{-\theta^TX}}$</p>
<p>**c) Advaced optimization</p>
<p><code>function [jVal, gradient] = costFunction(theta)</code></p>
<p><code>jVal =</code>[code to compute $J(\theta)$];</p>
<script type="math/tex; mode=display">J(\theta)= - \frac 1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p><code>gradient(1)</code> = [code to compute $\frac\partial{\partial\theta_0}J(\theta)$];</p>
<script type="math/tex; mode=display">\frac1m \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_0^{(i)}</script><p><code>gradient(2)</code> = [code to compute $\frac\partial{\partial\theta_1}J(\theta)$];</p>
<script type="math/tex; mode=display">\frac1m \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_1^{(i)}＋\frac\lambda m\theta_1</script><p><code>gradient(3)</code> = [code to compute $\frac\partial{\partial\theta_2}J(\theta)$];</p>
<script type="math/tex; mode=display">\frac1m \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_2^{(i)}＋\frac\lambda m\theta_2</script><p>…</p>
<p><code>gradient(n+1)</code> = [code to compute $\frac\partial{\partial\theta_n}J(\theta)$];</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;3-Logistic-Regression-and-Regularization&quot;&gt;&lt;a href=&quot;#3-Logistic-Regression-and-Regularization&quot; class=&quot;headerlink&quot; title=&quot;3. Logistic Regression and Regularization&quot;&gt;&lt;/a&gt;3. Logistic Regression and Regularization&lt;/h1&gt;&lt;p&gt;本周课程介绍了逻辑回归分析（分类问题求解）及正则化方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Logistic Regression" scheme="http://tech.liuxiaozhen.com/tags/Logistic-Regression/"/>
    
      <category term="Regularization" scheme="http://tech.liuxiaozhen.com/tags/Regularization/"/>
    
  </entry>
  
  <entry>
    <title>数据分析师养成计划（2）</title>
    <link href="http://tech.liuxiaozhen.com/2016/04/28/udacity-data-analyst-nanodegree-1/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/28/udacity-data-analyst-nanodegree-1/</id>
    <published>2016-04-28T08:59:53.000Z</published>
    <updated>2016-08-21T10:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录Udacity上的数据分析师Nanodegree的第一部分课程内容，包括最基本的统计学知识。</p>
<a id="more"></a>
<p><strong>本文是学习Udacity上的Nanodegree: Data Analyst的记录，自2016年4月24日始。</strong></p>
<p><strong>本系列全部文章见<a href="http://tech.liuxiaozhen.com/tags/Data-Analyst/">Tag: Data Analyst</a> 。</strong></p>
<h1 id="Project-1"><a href="#Project-1" class="headerlink" title="Project 1"></a>Project 1</h1><h2 id="Lesson-1-Intro-to-Research-Methods"><a href="#Lesson-1-Intro-to-Research-Methods" class="headerlink" title="Lesson 1 Intro to Research Methods"></a>Lesson 1 Intro to Research Methods</h2><p>这节课讲怎么做调查研究。定性地讲了采样，随机性、双盲实验、相关性与因果性，数据阐释等。</p>
<p>关键概念：</p>
<ul>
<li>Population &amp; Sample</li>
<li>Parameters &amp; Statistics</li>
<li>Sampling Error</li>
<li>Independent Variables, Dependent Variables, and Lurking Variables</li>
<li>Observations v.s. Experiments</li>
<li>相关性与因果性</li>
<li>双盲实验</li>
</ul>
<h2 id="Lesson-2-Visualizing-Data"><a href="#Lesson-2-Visualizing-Data" class="headerlink" title="Lesson 2: Visualizing Data"></a>Lesson 2: Visualizing Data</h2><p>这节课讲如何用直方图来呈现数据。</p>
<p>关键概念：</p>
<ul>
<li>Frequency, Relative Frequency, Proportions and Percentages</li>
<li>Histogram: numerical values</li>
<li>Bar Graph: categories</li>
<li>Bin size</li>
<li>Skewed Distribution<ul>
<li>Positively skewed distribution: scores with the lowest frequencies are on the right side of the distribution</li>
<li>Negatively skewed distribution: scores with the highest frequencies are on the left side of the distribution</li>
</ul>
</li>
<li>Uniform, bimode and normal distrubution</li>
</ul>
<h2 id="Lesson-3-Central-Tendancy"><a href="#Lesson-3-Central-Tendancy" class="headerlink" title="Lesson 3: Central Tendancy"></a>Lesson 3: Central Tendancy</h2><p>这节课主要讲mode, mean, median的概念，以及对于不同的分布，这些特征值有何特点，反映了什么样的性质。</p>
<h2 id="Lesson-4-Variability"><a href="#Lesson-4-Variability" class="headerlink" title="Lesson 4: Variability"></a>Lesson 4: Variability</h2><p>这节课讲了variance和standard deviation如何计算，以及normal distribution的一些属性。</p>
<p>目前为止课程都还很基础，暂时不会有什么大戏。我们还是来看看机器学习吧~<br>（待续）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录Udacity上的数据分析师Nanodegree的第一部分课程内容，包括最基本的统计学知识。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>数据分析师养成计划（1）</title>
    <link href="http://tech.liuxiaozhen.com/2016/04/27/udacity-data-analyst-nanodegree-0/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/27/udacity-data-analyst-nanodegree-0/</id>
    <published>2016-04-27T09:17:30.000Z</published>
    <updated>2016-08-21T10:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Udacity（在华名称“优达学城”）的“纳米学位”项目，及其数据分析师项目的基本情况。</p>
<a id="more"></a>
<p><strong>本文是学习Udacity上的Nanodegree: Data Analyst的记录，自2016年4月24日始。</strong></p>
<p><strong>本系列全部文章见<a href="http://tech.liuxiaozhen.com/tags/Data-Analyst/">Tag: Data Analyst</a> 。</strong></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在线学习平台Udacity去年刚刚完成D轮融资，估值超过10亿美元，成为在线教育第一家独角兽。我从2012年就开始关注和学习Udacity的课程。今年4月，Udacity以“优达学城”为名入华，一进入就跟滴滴、京东、新浪达成合作意向，为这些企业寻找和培养人才。</p>
<p>“纳米学位”（Nanodegreee, ND）是Udacity目前的主打产品，意图让学员通过9-12个月的业余学习（每月至少10小时投入）成为能够胜任某个IT技术岗位的人才（初级为主）。纳米学位的收费以月为单位，英文网站一般是$199/月，在华价格是￥980/月，这个价格主要用于支付一对一辅导，因为课程内容是免费的。</p>
<p>不过，你并不需要一万多元才能获得纳米学位，因为Udacity的考核是通过评估你提交的指定项目来完成。你完全可以先自学所有内容，做好项目，再付费学习。利用“一年内完成返还一半学费”的政策，最低只需一个月的价格就可以获得“学位证”（返还政策要求至少订阅两个月）。</p>
<p><strong>唯一的问题是：你能坚持学习吗？</strong></p>
<p>Udacity现有十余个“纳米学位”，包括数据分析师、机器学习工程师、前端工程师、后端工程师、iOS应用开发、安卓应用开发工程师等。部分课程已经被译成中文。</p>
<h1 id="课前准备"><a href="#课前准备" class="headerlink" title="课前准备"></a>课前准备</h1><p>我的数据分析基础还不错，所以数据分析师这个一半课程看起来都眼熟的ND就成了我的上手猎物……</p>
<p>Udacity改版之后，增加了不少贴心的功能，比如把某个Nanodegree需要多少时间完成都计算好，甚至连每一节课的时间都显示在学习计划中；确实为业余学习提供了不少便利。</p>
<p>然后我就在首月免费的蛊惑下订阅了这个学位啦。接着就不知不觉地看完了第一课。嗯，这不是统计学101的内容嘛，真的完全无压力好吗？</p>
<p>在纳米学位的欢迎视频中，一位知心大姐告诉我们：时间投入是成功完成学位的最重要因素，一定要在自己的日程中预留出学习时间。Udacity推荐每周至少花10小时学习纳米学位，且最好不要一次性学习10小时，分散到每天较好。</p>
<p>我算了算，每天学习3-4小时是能保证的。这样一周20小时都有余了。于是手一抖，又订阅了机器学习的纳米学位。花开两朵，各表一枝，关于机器学习的ND另文阐述。</p>
<p>下面就是学习数据分析师纳米学位的记录。</p>
<h1 id="Project-0"><a href="#Project-0" class="headerlink" title="Project 0"></a>Project 0</h1><p>这是一个选做项目，通过做这个项目可以上手未来本课程所需的工具。</p>
<h2 id="Python数据分析开发环境：Anaconda"><a href="#Python数据分析开发环境：Anaconda" class="headerlink" title="Python数据分析开发环境：Anaconda"></a>Python数据分析开发环境：Anaconda</h2><p>做这个项目首先要安装<a href="https://www.continuum.io/downloads" target="_blank" rel="external"> Anaconda </a>，以便愉快地使用Python。下载安装自不用提。官方提供的<a href="http://conda.pydata.org/docs/test-drive.html#managing-conda" target="_blank" rel="external"> Test Drive </a>会教一些基本配置方法。</p>
<h2 id="Python交互计算平台：IPython-Notebook"><a href="#Python交互计算平台：IPython-Notebook" class="headerlink" title="Python交互计算平台：IPython Notebook"></a>Python交互计算平台：IPython Notebook</h2><p>本项目用到了IPython Notebook。</p>
<p>这里<a href="http://mindonmind.github.io/2013/02/08/ipython-notebook-interactive-computing-new-era/" target="_blank" rel="external">引用一段</a>对IPython Notebook的介绍：</p>
<blockquote>
<p>IPython Notebook 既是一个交互计算平台，又是一个记录计算过程的「笔记本」。它由服务端和客户端两部分组成，其中服务端负责代码的解释与计算，而客户端负责与用户进行交互。服务端可以运行在本机也可以运行在远程服务器，包含负责运算的 IPython kernel (与 QT Console 的 kernel 相同) 以及一个 HTTP/S 服务器 (Tornado)。而客户端则是一个指向服务端地址的浏览器页面，负责接受用户的输入并负责渲染输出。</p>
</blockquote>
<p>如今IPython Notebook已经更名为jupyter（说实话感觉不如原来的名字）。</p>
<p>根据Udacity给出的下载链接下载ipynb文件后，在保存文件的路径下输入：</p>
<p><code>jupyter notebook Data_Analyst_ND_Project0.ipynb</code></p>
<p>即可启动本地服务器，在浏览器中查看和编辑该笔记本。</p>
<h2 id="Project-0-Chopsticks"><a href="#Project-0-Chopsticks" class="headerlink" title="Project 0: Chopsticks!"></a>Project 0: Chopsticks!</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>研究者想要找出成人和儿童使用筷子的最适宜长度。筷子使用的表现由夹起花生放到杯子里的花生个数来评估。</p>
<p><a href="http://www.ncbi.nlm.nih.gov/pubmed/15676839" target="_blank" rel="external">这项研究</a>发表在1991年的Applied Ergonomics上。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>实验进行了两次，分别对31名男性初级学院（相当于大学预科）学生和21名小学生进行。使用了长度分别为180, 210, 240, 270, 300 和330 mm的筷子。实验采用randomised complete block design. </p>
<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>本项目只采用成人组的数据。数据分为三列：夹取效率、实验对象编号、筷子长度。</p>
<p>每位成人都使用了一遍所有不同长度的筷子。</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>Independent Variable in the Experiment?</li>
<li>Dependent Variable in the Experiment?</li>
<li>Operational Definition of the Dependent Variable?</li>
<li>Controlled Variables? (list at least two)</li>
<li>Best chopstick length?</li>
<li>Relationship between length and efficiency?</li>
<li>Do you agree with the claim made by the researchers and why?</li>
</ol>
<p>（待续）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍Udacity（在华名称“优达学城”）的“纳米学位”项目，及其数据分析师项目的基本情况。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Udacity" scheme="http://tech.liuxiaozhen.com/tags/Udacity/"/>
    
      <category term="Nanodegree" scheme="http://tech.liuxiaozhen.com/tags/Nanodegree/"/>
    
      <category term="Statistics" scheme="http://tech.liuxiaozhen.com/tags/Statistics/"/>
    
      <category term="Data Analyst" scheme="http://tech.liuxiaozhen.com/tags/Data-Analyst/"/>
    
  </entry>
  
  <entry>
    <title>How to Use Git and GitHub</title>
    <link href="http://tech.liuxiaozhen.com/2016/04/06/udacity-GitHub-Notes/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/06/udacity-GitHub-Notes/</id>
    <published>2016-04-05T17:13:45.000Z</published>
    <updated>2016-11-08T13:51:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>Udacity课程之一，可能是最啰嗦的GitHub教程：）</p>
<a id="more"></a>
<p><strong>This note is for the Udacity course “How to Use Git and Github”.</strong></p>
<h2 id="Lesson-1"><a href="#Lesson-1" class="headerlink" title="Lesson 1"></a>Lesson 1</h2><h3 id="Find-differences-between-two-large-files"><a href="#Find-differences-between-two-large-files" class="headerlink" title="Find differences between two large files"></a>Find differences between two large files</h3><p><strong>Commands</strong>:</p>
<ul>
<li>Windows: FC</li>
<li>Mac/Linux: Diff</li>
</ul>
<blockquote>
<p>Reflect: How did viewing a diff between two versions help you spot the bug?<br>Answer: By looking at the differences between two versions, I know which changes have been made that caused the bug.</p>
</blockquote>
<p>Choose a text editor: Notepad++, Sublime, Atom, emacs, vim, etc. </p>
<h3 id="Versions"><a href="#Versions" class="headerlink" title="Versions"></a>Versions</h3><ul>
<li>Saving manual copies</li>
<li>Dropbox (periodically save versions automatically)</li>
<li>Google Docs (periodically save versions automatically)</li>
<li>Wikipedia (versions by different authors)</li>
<li>Git</li>
<li>SVN</li>
</ul>
<blockquote>
<p>Reflect: How could having easy access to the entire history of a file make you a more efficient programmer in the long term?<br>I don’t have to remember all the changes and the reasons for making them. With all the histories stored in exact forms, I can just look back at them when needed. Also, it helps me to learn from my own mistakes and allow me to do experiments without worrying about breaking things. </p>
</blockquote>
<p><strong>Feature Comparison Chart</strong></p>
<blockquote>
<p>Quiz: When to save: as a programer, when would you want to have version of your code saved?</p>
<ul>
<li>At regular intervals (e.g. every hour)</li>
<li>Whenever a large enough change is made (e.g. 50 lines)</li>
<li>Whenever there is a long pause in editing</li>
<li><strong>When you choose to save a version</strong></li>
</ul>
</blockquote>
<h3 id="Mannual-Commits"><a href="#Mannual-Commits" class="headerlink" title="Mannual Commits"></a>Mannual Commits</h3><p>Git requires a message with each commit</p>
<p>Cases that need a new commit:</p>
<ul>
<li>fix off-by-one bug</li>
<li>add cool new feature</li>
<li>improve user docs</li>
</ul>
<h3 id="Use-Git-to-View-History"><a href="#Use-Git-to-View-History" class="headerlink" title="Use Git to View History"></a>Use Git to View History</h3><p><strong>[Offline]</strong></p>
<ul>
<li>Each commit has an ID, an Autor, Date/Time, and a message</li>
<li><code>git diff ID1 ID2</code></li>
<li><p>Judgment Call</p>
<blockquote>
<p>Choosing when to commit is a judgment call, and it’s not always cut-and-dried. When choosing whether to commit, just keep in mind that each commit should have one clear, logical purpose, and you should never do too much work without committing.</p>
</blockquote>
</li>
</ul>
<h3 id="Commits-with-Multiple-Files"><a href="#Commits-with-Multiple-Files" class="headerlink" title="Commits with Multiple Files"></a>Commits with Multiple Files</h3><ul>
<li>A repository contains multiple files</li>
<li>Make changes in different files together and track in one commit<blockquote>
<p>Reflect: Why do you think some version control systems, like Git, allow saving multiple files in one commit, while others, like Google Docs, treat each file separately?<br>Because Git takes a mannual and logical approach, and keeps the records in one git file for each repo. For others, they keep the records as part of the information related to one file.</p>
</blockquote>
</li>
</ul>
<h3 id="Git-Commands"><a href="#Git-Commands" class="headerlink" title="Git Commands"></a>Git Commands</h3><p><strong>[Online]</strong></p>
<ul>
<li>Clone a repo <code>git clone _repo_url_</code><br>(<a href="https://github.com/udacity/asteroids.git" target="_blank" rel="external">https://github.com/udacity/asteroids.git</a>)</li>
<li><code>git log</code></li>
<li><code>git diff</code></li>
<li>Check commit: <code>git checkout _commit ID_</code></li>
</ul>
<h3 id="Making-Git-configurations"><a href="#Making-Git-configurations" class="headerlink" title="Making Git configurations"></a>Making Git configurations</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git config --global core.editor &quot;atom --wait&quot;</div><div class="line">git config --global push.default upstream</div><div class="line">git config --global merge.conflictstyle diff3</div></pre></td></tr></table></figure>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Why use Git</li>
<li>Git Setup</li>
<li>Git commands: clone, log, diff, checkout</li>
</ul>
<h2 id="Lesson-2"><a href="#Lesson-2" class="headerlink" title="Lesson 2"></a>Lesson 2</h2><h3 id="Initialize"><a href="#Initialize" class="headerlink" title="Initialize"></a>Initialize</h3><p><strong>[Offline]</strong></p>
<ul>
<li><code>git init</code></li>
<li>When a git is initialized, no commit is included</li>
<li>Check git status: <code>git status</code></li>
</ul>
<h3 id="Choosing-what-changes-to-commit"><a href="#Choosing-what-changes-to-commit" class="headerlink" title="Choosing what changes to commit"></a>Choosing what changes to commit</h3><ul>
<li>working directory -&gt; staging area -&gt; repository</li>
<li><code>git add _file_to_update_</code> will add named files to staging area</li>
<li>remove from staging area by using <code>git reset</code></li>
</ul>
<h3 id="Write-a-commit-message"><a href="#Write-a-commit-message" class="headerlink" title="Write a commit message"></a>Write a commit message</h3><ul>
<li><code>git commit</code> will open the editor</li>
<li>standard practice: write a message as if it is a command</li>
<li><code>git commit -m &quot;Commit message&quot;</code></li>
<li>commit message <a href="http://udacity.github.io/git-styleguide/" target="_blank" rel="external">style guide</a></li>
</ul>
<h3 id="Git-diff-revisited"><a href="#Git-diff-revisited" class="headerlink" title="Git diff revisited"></a>Git diff revisited</h3><ul>
<li><code>git diff</code> will compare the <strong>working directory</strong> with the <strong>staging area</strong></li>
<li><code>git diff --staged</code> will compare the <strong>staging area</strong> with the <strong>repository</strong></li>
<li><em>Be careful! <code>git reset --hard</code> is not reversable</em></li>
<li>Leave ‘detached HEAD’ state: <code>git checkout master</code></li>
</ul>
<h3 id="Create-and-commit-branches"><a href="#Create-and-commit-branches" class="headerlink" title="Create and commit branches"></a>Create and commit branches</h3><ul>
<li>One branch is enough: fix bug, new feature, update docs</li>
<li>More than one branch: experimental feature, Italian version</li>
<li>Most recent commit of a branch: tip of a branch</li>
<li>Show all the branches and the current branch is marked with * : <code>git branch</code></li>
<li>Create a branch: <code>git branch &quot;branch_name&quot;</code></li>
<li>Switch to a branch: <code>git checkout &quot;branch_name&quot;</code></li>
</ul>
<h3 id="Branches-for-Collaboration"><a href="#Branches-for-Collaboration" class="headerlink" title="Branches for Collaboration"></a>Branches for Collaboration</h3><p><strong>[Online]</strong></p>
<ul>
<li>remote branch</li>
<li>branch has parents “reachability”</li>
<li>checkout a commit as a new branch: <code>git checkout -b _new_branch_name_</code></li>
</ul>
<h3 id="Combining-Simple-Files"><a href="#Combining-Simple-Files" class="headerlink" title="Combining Simple Files"></a>Combining Simple Files</h3><ul>
<li>merged branch has both parents</li>
<li><code>git merge branch_1 branch_2</code></li>
<li><code>git show</code> show the diff of a commit with its parent</li>
<li>deleting branches: <code>git branch -d _branch_name_</code></li>
<li>Merge conflict: when both branches modify the same part of the files</li>
<li>Commit the conflict resolution</li>
<li><code>git log -n 1</code> show only 1 commit log</li>
</ul>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Initialize git</li>
<li>Create commits</li>
<li>Create and commit branches</li>
<li>Merge branches</li>
</ul>
<h2 id="Lesson-3"><a href="#Lesson-3" class="headerlink" title="Lesson 3"></a>Lesson 3</h2><h3 id="Sync-with-repositories-on-GitHub"><a href="#Sync-with-repositories-on-GitHub" class="headerlink" title="Sync with repositories on GitHub"></a>Sync with repositories on GitHub</h3><ul>
<li>Remote repo</li>
<li>push and pull</li>
<li>no working directory or staging area on GitHub side</li>
</ul>
<h3 id="Forking-a-Repository"><a href="#Forking-a-Repository" class="headerlink" title="Forking a Repository"></a>Forking a Repository</h3><ul>
<li>click fork button</li>
<li>= create a clone on GitHub instead of on your local computer.</li>
<li>Can add collaborators</li>
<li><code>git remote -v</code></li>
<li>push</li>
</ul>
<h3 id="Merge-the-Changes-Together"><a href="#Merge-the-Changes-Together" class="headerlink" title="Merge the Changes Together"></a>Merge the Changes Together</h3><ul>
<li>Fast-forward merge</li>
<li><code>git merge master origin/master</code></li>
<li>resolve conflict</li>
</ul>
<h3 id="Pull-request"><a href="#Pull-request" class="headerlink" title="Pull request"></a>Pull request</h3><ul>
<li>Ask for your pull request to be merged</li>
<li>after merging, delete branch</li>
</ul>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Clone and Fork</li>
<li>Merge and Pull request</li>
</ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Udacity课程之一，可能是最啰嗦的GitHub教程：）&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Machine Learning by Stanford University Week 2</title>
    <link href="http://tech.liuxiaozhen.com/2016/04/05/Machine-learning-W2/"/>
    <id>http://tech.liuxiaozhen.com/2016/04/05/Machine-learning-W2/</id>
    <published>2016-04-05T07:00:00.000Z</published>
    <updated>2016-08-21T10:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>Andrew Ng在Coursera上开设的机器学习课程第二周的内容，介绍了多元线性回归。</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course “Machine Learning” taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="Environment-Setup"><a href="#Environment-Setup" class="headerlink" title="Environment Setup"></a>Environment Setup</h2><p>Octave and MATLAB are preferred in machine learning. </p>
<p>For more information about Octave and MATLAB, see:<br><a href="https://www.coursera.org/learn/machine-learning/supplement/Mlf3e/more-octave-matlab-resources" target="_blank" rel="external">https://www.coursera.org/learn/machine-learning/supplement/Mlf3e/more-octave-matlab-resources</a></p>
<h2 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h2><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>为什么需要multiple features? 对于许多问题，影响预测结果的并不止一个因素，因此，需要多个变量来反映不同的影响因素。</p>
<p>Multiple Features如何用线性方程式来表示？</p>
<p>$h_\theta(x) = \theta_0 +\theta_1x_1+\theta_2x_2+…+\theta_nx_n = \theta^TX$</p>
<h3 id="Cost-Function-for-Multiple-Variables"><a href="#Cost-Function-for-Multiple-Variables" class="headerlink" title="Cost Function for Multiple Variables"></a>Cost Function for Multiple Variables</h3><p>$\theta$ is an $n+1$ -dimention vector</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1,...,\theta_n)=\frac1{2m}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2</script><h3 id="Gradient-Descent-for-Multiple-Variables"><a href="#Gradient-Descent-for-Multiple-Variables" class="headerlink" title="Gradient Descent for Multiple Variables"></a>Gradient Descent for Multiple Variables</h3><p>Repeat{</p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha\frac1m\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}</script><p>(simutaneously update $\theta_j$ for $j = 0,…,n$</p>
<p>notice that $x_0 = 1$)</p>
<p>}</p>
<h3 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h3><ul>
<li><p>Idea: get every feature into approximately a $-1\leq {x_i}\leq 1$ range</p>
</li>
<li><p>Mean normalization: replace $x_i$ with $x_i - \mu_i$ to make features have approximately zero mean (Do not apply to $x_0 = 1$)</p>
</li>
</ul>
<h3 id="Learning-Rate-alpha"><a href="#Learning-Rate-alpha" class="headerlink" title="Learning Rate $\alpha$"></a>Learning Rate $\alpha$</h3><ul>
<li>If $\alpha$ is too small: slow convergence.</li>
<li>If $\alpha$ is too large: $J(\theta)$ may not decrease on every iteration; may not converge. (slow converge also possible)</li>
<li>To choose $\alpha$, try: …,0.001, 0.01, 0.1, 1,…</li>
</ul>
<h3 id="Features-and-Polynomial-Regression"><a href="#Features-and-Polynomial-Regression" class="headerlink" title="Features and Polynomial Regression"></a>Features and Polynomial Regression</h3><ul>
<li><p>Polynomial regression example:    $\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3$; let $x_1 = x, x_2 = x^2, x_3 = x^3$</p>
</li>
<li><p>Other possiblities: $\theta_0+\theta_1x+\theta_2\sqrt{x}$, let $x_1 = x, x_2 = \sqrt{x}$</p>
</li>
</ul>
<h2 id="Computing-Parameters-Analytically"><a href="#Computing-Parameters-Analytically" class="headerlink" title="Computing Parameters Analytically"></a>Computing Parameters Analytically</h2><p>Suppose there are $m$ examples; $n$ features</p>
<p>$\theta = (X^TX)^{-1}X^Ty$<br>$(X^TX)^{-1}$ is inverse of matrix $X^TX$</p>
<p>if $m &lt; n$,<br>$X^TX$ may be non-inversible. </p>
<p>Therefore use <code>pinv</code> in Octave:<br><code>pinv(X&#39;*X)*X&#39;*y</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Gradient Discent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Need to choose $\alpha$</td>
<td>No need to choose $\alpha$</td>
</tr>
<tr>
<td>Needs many iterations</td>
<td>Don’t need to iterate</td>
</tr>
<tr>
<td>Works well even when $n$ is large</td>
<td>Need to compute $(X^TX)^{-1}$ ($O(n^3)$), slow if $n$ is very large</td>
</tr>
</tbody>
</table>
</div>
<p>Therefore, if n is less than 1000, use normal equation. Otherwise, use gradient descent.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Andrew Ng在Coursera上开设的机器学习课程第二周的内容，介绍了多元线性回归。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Linear Regression" scheme="http://tech.liuxiaozhen.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning by Stanford University Week 1</title>
    <link href="http://tech.liuxiaozhen.com/2016/03/10/Machine-learning-W1/"/>
    <id>http://tech.liuxiaozhen.com/2016/03/10/Machine-learning-W1/</id>
    <published>2016-03-10T12:41:19.000Z</published>
    <updated>2016-08-21T10:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>Andrew Ng在Coursera上开设的机器学习课程第一周的内容，介绍了机器学习基本定义，监督学习及无监督学习，以及单变量线性回归。</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course “Machine Learning” taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Definition of Machine Learning (Tom Mitchell): </p>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>
<h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><ul>
<li>Correct outputs are known. </li>
<li>Categorized into <strong>regression</strong> and <strong>classification</strong> problems</li>
<li>Regression problem: predict results within a continuous output</li>
<li>Classification problem: predict results in a discrete output (0 or 1)</li>
</ul>
<p>Example: </p>
<ul>
<li>Boston Housing price vs size (regression)</li>
<li>Breast cancer malignant vs benign (classification)<ul>
<li>more than one attribute</li>
</ul>
</li>
</ul>
<h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><ul>
<li>No known correct output (feedback)</li>
<li>Derive data structure by clustering the data based on relationships among the variables in the data</li>
<li>Examples: google story collection; cocktail party (distiguish two voices)</li>
</ul>
<h2 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h2><h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><p>Training set of housing prices (Portland, OR)<br>Notation:<br>$m$ = Number of training examples<br>$x$’s = “input” variable / features<br>$y$’s = “output” variable / “target” variable</p>
<p>$(x,y)$ - one training example</p>
<p>$(x^{(i)},y^{(i)})$ - $i^{th}$ training example</p>
<p><img src="Machine-learning-W1/Coursera-ml-w1-01.png" alt=""></p>
<p>How do we represent $h$?</p>
<script type="math/tex; mode=display">h_{\theta}(x) = \theta_0+\theta_1x</script><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p><strong>Idea</strong>: </p>
<p>Choose ${\theta<em>0}$, ${\theta_1}$ so that $h</em>{\theta}(x)$ is close to $y$ for our training examples $(x,y)$</p>
<h3 id="Cost-Function-Intuition"><a href="#Cost-Function-Intuition" class="headerlink" title="Cost Function - Intuition"></a>Cost Function - Intuition</h3><p>Hypothesis: <script type="math/tex">h_{\theta}(x) = \theta_0+\theta_1x</script></p>
<p>Parameters: $\theta_0, \theta_1$</p>
<p>Cost Function:</p>
<script type="math/tex; mode=display">J (\theta_0,\theta_1) = \frac 1{2m}{\sum_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2}</script><p>Goal:<br> $\text{minimize}\quad J(\theta_0,\theta_1)$</p>
<p><img src="Machine-learning-W1/Coursera-ml-w1-02.png" alt=""></p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>Have some function $J(\theta_0,\theta_1)$</p>
<p>want $\min \ J(\theta_0,\theta_1)$</p>
<p>Outline:</p>
<ul>
<li>start with some $\theta_0,\theta_1$</li>
<li>keep changing $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$ until we hopefully end up at a minimum</li>
</ul>
<h3 id="Gradient-descent-algorithm"><a href="#Gradient-descent-algorithm" class="headerlink" title="Gradient descent algorithm"></a>Gradient descent algorithm</h3><p>repeat until convergence {</p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial} {\partial \theta_j} J(\theta_0,\theta_1) $$     ( for $j = 0$ and $j =1$)

}

**Correct: simultaneous update $\theta_0$ and $\theta_1$**

$\alpha$ is the learning rate. 

- if $\alpha$ is too small, gradient descent can be slow. 

- if $\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. 

Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed. 

###Gradient Descent for Linear Regression

repeat until convergence {

$$\theta_0 := \theta_0 - \alpha \frac1m \sum_{i=1}^m \left( h_\theta (x^{(i)})-y^{(i)}\right)</script><script type="math/tex; mode=display">\theta_1 := \theta_1 - \alpha \frac1m \sum_{i=1}^m \left( h_\theta (x^{(i)})-y^{(i)} \right)\cdot x^{(i)}</script><p>}</p>
<h4 id="“Batch”-Gradient-Descent"><a href="#“Batch”-Gradient-Descent" class="headerlink" title="“Batch” Gradient Descent"></a>“Batch” Gradient Descent</h4><p>Each step of gradient descent uses all the training examples</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Andrew Ng在Coursera上开设的机器学习课程第一周的内容，介绍了机器学习基本定义，监督学习及无监督学习，以及单变量线性回归。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://tech.liuxiaozhen.com/tags/Machine-Learning/"/>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
      <category term="Linear Regression" scheme="http://tech.liuxiaozhen.com/tags/Linear-Regression/"/>
    
  </entry>
  
  <entry>
    <title>Python for Everybody by University of Michigan (1)</title>
    <link href="http://tech.liuxiaozhen.com/2015/12/06/Python-for-Everybody-1/"/>
    <id>http://tech.liuxiaozhen.com/2015/12/06/Python-for-Everybody-1/</id>
    <published>2015-12-06T03:51:47.000Z</published>
    <updated>2016-08-21T10:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍了编程基本知识，Python安装，表达式，变量等。</p>
<a id="more"></a>
<p><strong>This note is for the first course of University of Michigan online series “Python for Everybody Specialization” taught by Professor Charles Severance on Coursera.org, 2015 October session.</strong></p>
<h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Why-Program"><a href="#Why-Program" class="headerlink" title="Why Program?"></a>Why Program?</h2><h2 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h2><ul>
<li>CPU</li>
<li>I/O</li>
<li>Main sMemory (RAM)</li>
<li>Secondary Memory</li>
</ul>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><ul>
<li>Vocabulary/Words - Variables and Reserved words</li>
<li>Sentence structure - valid syntax patterns</li>
<li>Story structure - constructing a program for a purpose</li>
</ul>
<h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Install-Python"><a href="#Install-Python" class="headerlink" title="Install Python"></a>Install Python</h2><ul>
<li>IDLE</li>
<li>Python Downloads: <a href="https://www.python.org/downloads/" target="_blank" rel="external">https://www.python.org/downloads/</a></li>
<li>Text Editor</li>
</ul>
<h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Simple-program"><a href="#Simple-program" class="headerlink" title="Simple program"></a>Simple program</h2><ul>
<li>Sequential steps</li>
<li>Conditional steps</li>
<li>Repeated steps</li>
</ul>
<h2 id="Think-like-a-program"><a href="#Think-like-a-program" class="headerlink" title="Think like a program"></a>Think like a program</h2><ul>
<li>How to find the largest number?</li>
</ul>
<h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Expressions"><a href="#Expressions" class="headerlink" title="Expressions"></a>Expressions</h2><ul>
<li>Variables and Constants <ul>
<li>Constants</li>
<li>Python variable name rules: start with letters, case sensitive</li>
</ul>
</li>
<li>Sentences or lines<ul>
<li>Assignment statement: <code>x = 2</code></li>
<li>Assignment with expression: <code>x = x + 2</code> <ul>
<li>Print statement: <code>print x</code></li>
</ul>
</li>
</ul>
</li>
<li>Operator<ul>
<li>Numeric expressions: +, -, <em>, /, *</em>, %</li>
<li>Operator precedence rules: parenthesis, power, multiplication, addition, left to right<ul>
<li>Reserved words</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h2><ul>
<li>Type<ul>
<li>Interger</li>
<li>Float</li>
<li>String</li>
<li>…</li>
</ul>
</li>
<li>Type conversions<ul>
<li>Type of x: <code>type(x)</code></li>
<li>Convert x to an interger <code>int(x)</code></li>
<li>Convert x to a float<code>float(x)</code></li>
</ul>
</li>
<li>User Input<ul>
<li><code>nam = raw_input(&quot;Who are you?&quot;)</code></li>
</ul>
</li>
<li>Comments in Python: anything after <code>#</code></li>
<li>Variable naming style</li>
</ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍了编程基本知识，Python安装，表达式，变量等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://tech.liuxiaozhen.com/tags/Python/"/>
    
      <category term="Coursera" scheme="http://tech.liuxiaozhen.com/tags/Coursera/"/>
    
  </entry>
  
  <entry>
    <title>启</title>
    <link href="http://tech.liuxiaozhen.com/2015/04/03/start/"/>
    <id>http://tech.liuxiaozhen.com/2015/04/03/start/</id>
    <published>2015-04-03T09:17:05.000Z</published>
    <updated>2016-08-21T11:55:35.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="start/dune.jpeg" alt=""></p>
<p>从小学就开始编程，但是仅限于好玩；在学习科研中偶尔涉及编程，也不过是简短的几十行。从2009年入手iPod Touch至今，关于App的想法无数，却始终没有成为一个真正的创造者。</p>
<p>tinyfool在去年的一篇<a href="http://www.jianshu.com/p/c27906bb8061" target="_blank" rel="external">《寻找和突破心障》</a>中说：</p>
<blockquote>
<p>人生中总有些障碍阻挡着你去做你想做的事情，但是这些障碍里面有一些是物理障碍，如果你真的没有时间和金钱，不去旅游也就不去旅游了。如果你有时间、有金钱，也有一个说走就走的心，但是你哪里都没有去过，那就是心障。</p>
</blockquote>
<p>关于编程，我想我并非真的完全没有时间，而仅仅出于心障而没有做。近年来，在效率和心智方面收获许多，对于开启这样一段旅程慢慢有了信心。写笔记，更多的是希望靠输出的过程来加强学习效果。若能在比特世界，无意间给一些人帮助，则幸甚至哉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;start/dune.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从小学就开始编程，但是仅限于好玩；在学习科研中偶尔涉及编程，也不过是简短的几十行。从2009年入手iPod Touch至今，关于App的想法无数，却始终没有成为一个真正的创造者。&lt;/p&gt;
&lt;
    
    </summary>
    
    
  </entry>
  
</feed>
