<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Machine Learning By Stanford University Week 3 | 刘虓震的技术笔记</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="3. Logistic Regression and Regularization&amp;#x672C;&amp;#x5468;&amp;#x8BFE;&amp;#x7A0B;&amp;#x4ECB;&amp;#x7ECD;&amp;#x4E86;&amp;#x903B;&amp;#x8F91;&amp;#x56DE;&amp;#x5F52;&amp;#x5206;&amp;#x6790;&amp;#xFF08;&amp;#x5206;&amp;#x7C7B;&amp;#x95EE;&amp;#x9898;&amp;#x6C42;&amp;#x89E3">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning By Stanford University Week 3">
<meta property="og:url" content="http://tech.liuxiaozhen.com/2016/05/14/Machine-Learning-W3/index.html">
<meta property="og:site_name" content="刘虓震的技术笔记">
<meta property="og:description" content="3. Logistic Regression and Regularization&amp;#x672C;&amp;#x5468;&amp;#x8BFE;&amp;#x7A0B;&amp;#x4ECB;&amp;#x7ECD;&amp;#x4E86;&amp;#x903B;&amp;#x8F91;&amp;#x56DE;&amp;#x5F52;&amp;#x5206;&amp;#x6790;&amp;#xFF08;&amp;#x5206;&amp;#x7C7B;&amp;#x95EE;&amp;#x9898;&amp;#x6C42;&amp;#x89E3">
<meta property="og:image" content="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-01.png">
<meta property="og:image" content="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-02.png">
<meta property="og:image" content="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-03.png">
<meta property="og:image" content="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-04.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning By Stanford University Week 3">
<meta name="twitter:description" content="3. Logistic Regression and Regularization&amp;#x672C;&amp;#x5468;&amp;#x8BFE;&amp;#x7A0B;&amp;#x4ECB;&amp;#x7ECD;&amp;#x4E86;&amp;#x903B;&amp;#x8F91;&amp;#x56DE;&amp;#x5F52;&amp;#x5206;&amp;#x6790;&amp;#xFF08;&amp;#x5206;&amp;#x7C7B;&amp;#x95EE;&amp;#x9898;&amp;#x6C42;&amp;#x89E3">
  
    <link rel="alternative" href="/atom.xml" title="刘虓震的技术笔记" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">刘虓震的技术笔记</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Live in the Future, then Build What&#39;s Missing</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">首页</a>
        
          <a class="main-nav-link" href="http://tech.liuxiaozhen.com">技术</a>
        
          <a class="main-nav-link" href="/archives">所有文章</a>
        
          <a class="main-nav-link" href="/about">关于</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://tech.liuxiaozhen.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Machine-Learning-W3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/05/14/Machine-Learning-W3/" class="article-date">
  <time datetime="2016-05-14T15:22:21.000Z" itemprop="datePublished">2016-05-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Machine Learning By Stanford University Week 3
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="3-Logistic-Regression-and-Regularization"><a href="#3-Logistic-Regression-and-Regularization" class="headerlink" title="3. Logistic Regression and Regularization"></a>3. Logistic Regression and Regularization</h1><p>&#x672C;&#x5468;&#x8BFE;&#x7A0B;&#x4ECB;&#x7ECD;&#x4E86;&#x903B;&#x8F91;&#x56DE;&#x5F52;&#x5206;&#x6790;&#xFF08;&#x5206;&#x7C7B;&#x95EE;&#x9898;&#x6C42;&#x89E3;&#xFF09;&#x53CA;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#x3002;</p>
<a id="more"></a>
<p><strong>This note is for the Stanford University online course &#x201C;Machine Learning&#x201D; taught by Andrew Ng on Coursera.org, 2016 March session.</strong></p>
<h2 id="3-1-Logistic-Regression"><a href="#3-1-Logistic-Regression" class="headerlink" title="3.1 Logistic Regression"></a>3.1 Logistic Regression</h2><h3 id="3-1-1-Classification-Problems-and-Logistic-Regression-Model"><a href="#3-1-1-Classification-Problems-and-Logistic-Regression-Model" class="headerlink" title="3.1.1 Classification Problems and Logistic Regression Model"></a>3.1.1 Classification Problems and Logistic Regression Model</h3><p><strong>a) Classification Problems</strong></p>
<ul>
<li>Email&#xFF1A; spam/not spam</li>
<li>Online transactions: froudulent or not</li>
<li>Tumor: malignant/benign</li>
</ul>
<p>$y\in{0,1}$</p>
<ul>
<li>0: &#x201C;Negative Class&#x201D;</li>
<li>1: &#x201C;Positive Class&#x201D;</li>
</ul>
<p>Use threshold classifier output $h_\theta(x)$ at 0.5</p>
<ul>
<li>If $h_\theta(x) \geq 0.5$, predict &#x201C;y=1&#x201D;</li>
<li>If $h_\theta(x) &lt; 0.5$, predict &#x201C;y=0&#x201D; </li>
</ul>
<p><strong>b) Logistic Regression Model</strong><br>Want $0\leq h_\theta(x) \leq 1$</p>
<p>$h_\theta(x) = g(\theta^Tx)$<br>The sigmoid function: $g(z) = \frac 1{1+e^{-z}}$</p>
<script type="math/tex; mode=display">h_\theta(x) =\frac 1{1+e^{-\theta^Tx}}</script><p>whenever $z \geq 0$, or $\theta^Tx \geq 0$,y =1</p>
<h3 id="3-1-2-Decision-Boundary"><a href="#3-1-2-Decision-Boundary" class="headerlink" title="3.1.2 Decision Boundary"></a>3.1.2 Decision Boundary</h3><p><strong>a) Decision Boundary</strong></p>
<p>$h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2)$</p>
<p><strong>b) Non-linear decision boundaries</strong></p>
<p>$h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2)$</p>
<p>Predict &#x201C;y=1&#x201D; if $-1+x_1^2+x_2^2 \geq 0$</p>
<h2 id="3-1-3-Cost-Function"><a href="#3-1-3-Cost-Function" class="headerlink" title="3.1.3 Cost Function"></a>3.1.3 Cost Function</h2><p>Logistic regression:</p>
<script type="math/tex; mode=display">\text{Cost}(h_\theta(x^{(i)}),y^{(i)}) = \frac12\left(h_\theta(x^{(i)})-y^{(i)}\right)^2</script><p>&#x201C;non-convex&#x201D; v.s. &#x201C;convex&#x201D;</p>
<script type="math/tex; mode=display">\text{Cost}(h_\theta(x),y)=\begin{cases}\quad-\log(h_\theta(x)) 
\quad\text{if } y = 1\\
-\log(1-h_\theta(x))\quad\text{if }y =0
\end{cases}</script><p>Cost = 0 if y=1, $h<em>\theta(x)=1$<br>But as $h</em>\theta(x)\rightarrow 0$, Cost $\rightarrow \infty$</p>
<p>More compact way:</p>
<p>$\text{Cost}(h<em>\theta(x),y)=-y\log\left(h</em>\theta(x)\right)-(1-y)\log\left(1-h_\theta(x)\right)<br>$</p>
<script type="math/tex; mode=display">J(\theta)= - \frac 1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]</script><p>To fit parameters $\theta$:<br>$\min_\theta J (\theta)$</p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\sum_{i=1}^m \left( h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(j)}</script><p>}</p>
<p>Algorithm looks identical to linear regression!</p>
<h3 id="3-1-4-Optimization-algorithm"><a href="#3-1-4-Optimization-algorithm" class="headerlink" title="3.1.4 Optimization algorithm"></a>3.1.4 Optimization algorithm</h3><ul>
<li>Gradient descent</li>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>Advantages:</p>
<ul>
<li><p>No need to manually pick $\alpha$</p>
</li>
<li><p>Often faster than gradient descent</p>
</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>More complex</li>
</ul>
<h3 id="3-1-5-Multiclass-Classification"><a href="#3-1-5-Multiclass-Classification" class="headerlink" title="3.1.5 Multiclass Classification"></a>3.1.5 Multiclass Classification</h3><p><strong>a) Problem examples</strong></p>
<ul>
<li>Email foldering/tagging: work, friends, family, hobby</li>
<li>Medical diagrams: not ill, cold, flu</li>
<li>Weather: sunny, cloudy, rain, snow</li>
</ul>
<p><strong>b) One-vs-all (one-vs-rest)</strong><br><img src="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-01.png" alt=""></p>
<p>&#x5C06;&#x9700;&#x8981;&#x5206;&#x4E3A;N&#x7C7B;&#x7684;&#x6570;&#x636E;&#x62C6;&#x5206;&#x6210; N&#x4E2A;0/1&#xFF08;&#x662F;&#xFF0F;&#x5426;&#xFF09;&#x95EE;&#x9898;&#x3002;</p>
<p>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$.</p>
<p>On a new input $x$, to make a prediction, pick the class $i$ that maximizes $\max h_\theta^{(i)}(x)$.</p>
<h2 id="3-2-Regularization-&#x6B63;&#x5219;&#x5316;"><a href="#3-2-Regularization-&#x6B63;&#x5219;&#x5316;" class="headerlink" title="3.2 Regularization &#x6B63;&#x5219;&#x5316;"></a>3.2 Regularization &#x6B63;&#x5219;&#x5316;</h2><h3 id="3-2-1-Overfitting-Problems-&#x8FC7;&#x62DF;&#x5408;&#x95EE;&#x9898;"><a href="#3-2-1-Overfitting-Problems-&#x8FC7;&#x62DF;&#x5408;&#x95EE;&#x9898;" class="headerlink" title="3.2.1 Overfitting Problems &#x8FC7;&#x62DF;&#x5408;&#x95EE;&#x9898;"></a>3.2.1 Overfitting Problems &#x8FC7;&#x62DF;&#x5408;&#x95EE;&#x9898;</h3><p><strong>a) Example: Linear Regression (housing prices)</strong></p>
<p><img src="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-02.png" alt=""></p>
<p>&#x5BF9;&#x4E8E;&#x623F;&#x5C4B;&#x4EF7;&#x683C;&#x7684;&#x4E09;&#x79CD;&#x62DF;&#x5408;&#x66F2;&#x7EBF;&#x4E0E;&#x5B9E;&#x9645;&#x6570;&#x636E;&#x7684;&#x7B26;&#x5408;&#x60C5;&#x51B5;&#xFF1A;</p>
<ul>
<li>&#x4E00;&#x5143;&#x7EBF;&#x6027;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;underfit, high bias</li>
<li>&#x4E00;&#x5143;&#x4E8C;&#x6B21;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;just right</li>
<li>&#x4E00;&#x5143;&#x56DB;&#x6B21;&#x8868;&#x8FBE;&#x5F0F;&#xFF1A;overfit, high variance</li>
</ul>
<p><strong>Overfitting:</strong> If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples (predict prices on new examples).</p>
<p><strong>b) Example: Logistic Regression</strong><br><img src="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-03.png" alt=""></p>
<p><strong>Addressing overfitting:</strong></p>
<ul>
<li>Reduce number of features:<ul>
<li>Manually select which features to keep</li>
<li>Model selection algorithm (later in course)</li>
</ul>
</li>
<li>Regularization:<ul>
<li>Keep all the features, but reduce magnitude/values of parameters $\theta_j$.</li>
<li>Works well when we have a lot of features, each of which contributes a bit to predicting $y$.</li>
</ul>
</li>
</ul>
<h3 id="3-2-2-Cost-Function"><a href="#3-2-2-Cost-Function" class="headerlink" title="3.2.2 Cost Function"></a>3.2.2 Cost Function</h3><p><strong>a) Intuition</strong></p>
<p><img src="/2016/05/14/Machine-Learning-W3/Coursera-ml-w3-04.png" alt=""></p>
<p>Suppose we penalize and make $\theta_3,\theta_4$ really small.</p>
<script type="math/tex; mode=display">\min_\theta\frac1{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2 +1000\theta_3^2+1000\theta_4^2</script><p>$\theta_3,\theta_4$ will be very small.</p>
<p><strong>b) Regularization</strong></p>
<p>Small values for parameters $\theta_0, \theta_1, &#x2026;, \theta_n$</p>
<ul>
<li>&#x201C;Simpler&#x201D; hypothesis</li>
<li>Less prone to overfitting</li>
</ul>
<p><strong>Cost Function</strong></p>
<script type="math/tex; mode=display">
J(\theta) = \frac1{2m}\left[\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+\lambda\sum_{j=1}^n\theta_j^2\right]</script><p>$\theta_0$ is excluded as a convention.</p>
<p>If $\lambda$ is too large, we might have underfitting problems. </p>
<h3 id="3-2-3-Regularized-Linear-Regression"><a href="#3-2-3-Regularized-Linear-Regression" class="headerlink" title="3.2.3 Regularized Linear Regression"></a>3.2.3 Regularized Linear Regression</h3><p><strong>a) Gradient descent</strong></p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\left[\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}+\frac{\lambda}m\theta_j\right]</script><p>}</p>
<script type="math/tex; mode=display">
\theta_j := \theta_j(1-\alpha\frac{\lambda}m)-\alpha\frac1m\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}</script><p>$1-\alpha\frac{\lambda}m&lt;1$. Therefore, use $\theta_j^2$ as an approximation</p>
<p><strong>b) Normal equation</strong></p>
<script type="math/tex; mode=display">
X = \begin{bmatrix}
(x^{(1)})^T\\
...\\
(x^{(m)})^T\\
\end{bmatrix}</script><script type="math/tex; mode=display">
y = \begin{bmatrix}
(y^{(1)})^T\\
...\\
(y^{(m)})^T\\
\end{bmatrix}</script><p>To minimize $J(\theta)$</p>
<script type="math/tex; mode=display">\theta = \left(X^TX+\lambda \begin{bmatrix}
0&0&0&...&0\\
0&1&0&...&0\\
0&0&1&...&0\\
...&...&...&1&...\\
0&0&0&...&1\\
\end{bmatrix}\right)^{-1}X^Ty</script><p><strong>c) Non-invertibility (optional/advanced)</strong></p>
<p>Suppose $m\leq n$,<br>(where m: number of examples; n: number of features)</p>
<script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>$X^TX$ is non-invertible/singular. In Octave/MATLAB, use <code>pinv</code> instead of <code>inv</code>.</p>
<p>If $\lambda \gt 0,$</p>
<script type="math/tex; mode=display">
\theta = \left(X^TX+\lambda \begin{bmatrix}
0&0&0&...&0\\
0&1&0&...&0\\
0&0&1&...&0\\
...&...&...&1&...\\
0&0&0&...&1\\
\end{bmatrix}\right)^{-1}X^Ty</script><h3 id="3-2-4-Regularized-Logistic-Regression"><a href="#3-2-4-Regularized-Logistic-Regression" class="headerlink" title="3.2.4 Regularized Logistic Regression"></a>3.2.4 Regularized Logistic Regression</h3><p><strong>a) Cost function:</strong></p>
<script type="math/tex; mode=display">J(\theta)= - \frac 1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p><strong>b) Gradient descent</strong></p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\left[\frac1m \sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_j^{(i)}+\frac{\lambda}m\theta_j\right]</script><p>}</p>
<p>where $h_\theta(x)=\frac1{1+e^{-\theta^TX}}$</p>
<p>**c) Advaced optimization</p>
<p><code>function [jVal, gradient] = costFunction(theta)</code></p>
<p><code>jVal =</code>[code to compute $J(\theta)$];</p>
<script type="math/tex; mode=display">J(\theta)= - \frac 1m\left[\sum_{i=1}^m y^{(i)}\log h_\theta\left(x^{(i)}\right)+\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p><code>gradient(1)</code> = [code to compute $\frac\partial{\partial\theta_0}J(\theta)$];</p>
<script type="math/tex; mode=display">\frac1m \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_0^{(i)}</script><p><code>gradient(2)</code> = [code to compute $\frac\partial{\partial\theta_1}J(\theta)$];</p>
<script type="math/tex; mode=display">\frac1m \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_1^{(i)}＋\frac\lambda m\theta_1</script><p><code>gradient(3)</code> = [code to compute $\frac\partial{\partial\theta_2}J(\theta)$];</p>
<script type="math/tex; mode=display">\frac1m \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_2^{(i)}＋\frac\lambda m\theta_2</script><p>&#x2026;</p>
<p><code>gradient(n+1)</code> = [code to compute $\frac\partial{\partial\theta_n}J(\theta)$];</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://tech.liuxiaozhen.com/2016/05/14/Machine-Learning-W3/" data-id="civ9p4dax001jp3wcw497rpgw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Coursera/">Coursera</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Regularization/">Regularization</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/06/12/Machine-Learning-W5/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Machine Learning By Stanford University Week 5
        
      </div>
    </a>
  
  
    <a href="/2016/04/28/udacity-data-analyst-nanodegree-1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">数据分析师养成计划（2）</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Coursera/">Coursera</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analyst/">Data Analyst</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Learning-Curves/">Learning Curves</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Regression/">Linear Regression</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MLND/">MLND</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model-Selection/">Model Selection</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nanodegree/">Nanodegree</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Overfitting/">Overfitting</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Precision/">Precision</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recall/">Recall</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regularization/">Regularization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Udacity/">Udacity</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Coursera/" style="font-size: 20px;">Coursera</a><a href="/tags/Data-Analyst/" style="font-size: 10px;">Data Analyst</a><a href="/tags/Learning-Curves/" style="font-size: 10px;">Learning Curves</a><a href="/tags/Linear-Regression/" style="font-size: 15px;">Linear Regression</a><a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a><a href="/tags/MLND/" style="font-size: 10px;">MLND</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a><a href="/tags/Model-Selection/" style="font-size: 10px;">Model Selection</a><a href="/tags/Nanodegree/" style="font-size: 15px;">Nanodegree</a><a href="/tags/Neural-Networks/" style="font-size: 10px;">Neural Networks</a><a href="/tags/Overfitting/" style="font-size: 10px;">Overfitting</a><a href="/tags/Precision/" style="font-size: 10px;">Precision</a><a href="/tags/Python/" style="font-size: 10px;">Python</a><a href="/tags/Recall/" style="font-size: 10px;">Recall</a><a href="/tags/Regularization/" style="font-size: 10px;">Regularization</a><a href="/tags/Statistics/" style="font-size: 15px;">Statistics</a><a href="/tags/Udacity/" style="font-size: 15px;">Udacity</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archive/2016/07/">July 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archive/2016/06/">June 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archive/2016/05/">May 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archive/2016/04/">April 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archive/2016/03/">March 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archive/2015/12/">December 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archive/2015/04/">April 2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/07/06/Udacity-MLND-01/">机器学习工程师之路（1）</a>
          </li>
        
          <li>
            <a href="/2016/06/25/Machine-Learning-W6/">Machine Learning By Stanford University Week 6</a>
          </li>
        
          <li>
            <a href="/2016/06/12/Machine-Learning-W5/">Machine Learning By Stanford University Week 5</a>
          </li>
        
          <li>
            <a href="/2016/05/14/Machine-Learning-W3/">Machine Learning By Stanford University Week 3</a>
          </li>
        
          <li>
            <a href="/2016/04/28/udacity-data-analyst-nanodegree-1/">数据分析师养成计划（2）</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Liu Xiaozhen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="http://tech.liuxiaozhen.com" class="mobile-nav-link">技术</a>
  
    <a href="/archives" class="mobile-nav-link">所有文章</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>

<!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config(null);
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
</body>
</html>